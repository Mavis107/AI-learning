{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Resnet18 to train on CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10\n",
    "- Consists of 32x32 colour images in 10 classes\n",
    "- 50000 training images + 10000 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTY8C-LBhDTX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image normalization : \n",
    "#### 1. min/max normalization: 縮到 0-1 或 -1-1 之間，通常是 input range 已知的情況可用，output = input / 255 <br>\n",
    "$x_{scaled}=\\frac {x-x_{min}} {x_{max}-x_{min}}$\n",
    "- transforms.ToTensor(): range(0, 255) -> range(0.0, 1.0)  \n",
    "\n",
    "#### 2. Standardization: 將 sampled dataset 的 mean 和 std 轉換成接近於 0 和 1，以此減少偏差，避免被某部分資料支配，通常是用在 Input range 未知的情況，以採樣的方式來取得。\n",
    "- mean -> 0 ：unbiased的data更有利於model學習\n",
    "- std -> 1 ：減緩梯度消失和梯度爆炸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "70e8542b6c6b4526bd904077c7693b2d",
      "667dd381dd9f48df951ac521eb3e7a3e",
      "d9c4be8524af4e09aa38fde9842bf2b4",
      "f937b15f2da944e483e1fa58b28526d3",
      "184435fbcabc4db88b069bc1b5d5188b",
      "a8b37ed735d54c689dd37c32e68cb3d1",
      "9b9c2d61e8664e2eb945d79481e199e5",
      "24ccea8108d74cf3a0e28e79731e28f6",
      "7d07480af9504349bd46804fe5df7020",
      "89505cba3db942b6ae37a34138c1d614",
      "91152223b154456888596988b459022a"
     ]
    },
    "id": "3LzusjVfhxJ3",
    "outputId": "159af5d5-dbe7-4454-8856-d736441f7d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e8542b6c6b4526bd904077c7693b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "[0.49211264 0.48351443 0.44780117] [0.24790427 0.24408253 0.26234666]\n"
     ]
    }
   ],
   "source": [
    "# 計算normalization需要的 mean & std\n",
    "def get_mean_std(dataset, ratio=0.3):\n",
    "    # Get mean and std by sample ratio\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=int(len(dataset)*ratio), shuffle=True, num_workers=2)\n",
    "\n",
    "    data = next(iter(dataloader))[0]     # get the first iteration data\n",
    "    mean = np.mean(data.numpy(), axis=(0,2,3))\n",
    "    std = np.std(data.numpy(), axis=(0,2,3))\n",
    "    return mean, std\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_mean, train_std = get_mean_std(train_dataset)\n",
    "\n",
    "print(train_mean, train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "- 為了在 training 樣本固定的情況下，藉由不改變被辨識物件特性(例如 classification種類)，對 image 做一些改動，來讓 training data 更多元化\n",
    "- 將圖片進行旋轉、調整大小、比例尺寸，或改變亮度色溫、翻轉、加入 Gaussian noise等處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LkcHJBCRpiwq"
   },
   "outputs": [],
   "source": [
    "# 計算 Gaussian noise # 將noise取一個高斯分佈的值並疊加到原圖上，可增加顆粒感\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation & normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5NGnL4rP6ofO"
   },
   "outputs": [],
   "source": [
    "data_statistics = ((0.491, 0.482, 0.446), (0.247, 0.243, 0.261)) # (mean ,std) in 3 channels\n",
    "transform_train = transforms.Compose([\n",
    "    # Data augmentation\n",
    "    # transforms.RandomCrop(32, padding = 4, padding_mode= 'reflect'),\n",
    "    transforms.RandomHorizontalFlip(),       # Flips the image w.r.t horizontal axis\n",
    "    # transforms.RandomRotation((-7,7)),     # Rotates the image to a specified angel\n",
    "    # transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), # Performs actions like zooms, change shear angles.\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n",
    "    # Data normalization\n",
    "    transforms.ToTensor(),    # C * H * W -> [0,1]\n",
    "    # [-1,1] -> standardization: (image - train_mean) / train_std\n",
    "    transforms.Normalize(*data_statistics, inplace = True),  \n",
    "    # transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261))\n",
    "    # add Gaussian noise\n",
    "    # AddGaussianNoise(0., 1.),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*data_statistics, inplace = True)     \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLzEAksm-xeZ",
    "outputId": "c482ad85-6ff5-45fb-f2e3-94afc103febd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oaJyYUXj6rNW",
    "outputId": "8c793086-57e9-41d7-f023-4e263b417e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomCrop(size=(32, 32), padding=4)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.491, 0.482, 0.446), std=(0.247, 0.243, 0.261))\n",
      "               AddGaussianNoise(mean=0.0, std=1.0)\n",
      "           )\n",
      "train_dataset length:  50000\n",
      "classes:  ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "# 檢查training dataset長怎麼樣 \n",
    "print(train_dataset)\n",
    "print(\"train_dataset length: \", len(train_dataset))\n",
    "print(\"classes: \", train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BaIP41qxYr2R"
   },
   "outputs": [],
   "source": [
    "# setting parameter\n",
    "EPOCH = 200     \n",
    "BATCH_SIZE = 150    \n",
    "lr = 0.05        \n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uya8Cq7SSzuV",
    "outputId": "f3019f51-412d-419b-b8b7-c72b553d4a4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length:  45000\n",
      "val length:  5000\n",
      "test length:  10000\n"
     ]
    }
   ],
   "source": [
    "# split validation dataset\n",
    "torch.manual_seed(43)     # 確保每次獲得相同的驗證集\n",
    "val_size = 5000           # 取 5000張驗證集(0.1 of trainset)\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "print(\"train length: \", len(train_dataset))\n",
    "print(\"val length: \", len(val_dataset))\n",
    "print(\"test length: \", len(test_dataset))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)   #生成batch \n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5L4v2GB4fAde"
   },
   "outputs": [],
   "source": [
    "# 3x3 convolution\n",
    "def conv3x3(in_channels, out_channels, stride = 1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ODGtOo1Hi8Jn",
    "outputId": "0dd9631e-9708-4da5-9abd-d430c63b8032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             432\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "              ReLU-3           [-1, 16, 32, 32]               0\n",
      "            Conv2d-4           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-5           [-1, 16, 32, 32]              32\n",
      "              ReLU-6           [-1, 16, 32, 32]               0\n",
      "            Conv2d-7           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-8           [-1, 16, 32, 32]              32\n",
      "              ReLU-9           [-1, 16, 32, 32]               0\n",
      "    ResidualBlock-10           [-1, 16, 32, 32]               0\n",
      "           Conv2d-11           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-12           [-1, 16, 32, 32]              32\n",
      "             ReLU-13           [-1, 16, 32, 32]               0\n",
      "           Conv2d-14           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-15           [-1, 16, 32, 32]              32\n",
      "             ReLU-16           [-1, 16, 32, 32]               0\n",
      "    ResidualBlock-17           [-1, 16, 32, 32]               0\n",
      "           Conv2d-18           [-1, 32, 16, 16]           4,608\n",
      "      BatchNorm2d-19           [-1, 32, 16, 16]              64\n",
      "             ReLU-20           [-1, 32, 16, 16]               0\n",
      "           Conv2d-21           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-22           [-1, 32, 16, 16]              64\n",
      "           Conv2d-23           [-1, 32, 16, 16]           4,608\n",
      "      BatchNorm2d-24           [-1, 32, 16, 16]              64\n",
      "             ReLU-25           [-1, 32, 16, 16]               0\n",
      "    ResidualBlock-26           [-1, 32, 16, 16]               0\n",
      "           Conv2d-27           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-28           [-1, 32, 16, 16]              64\n",
      "             ReLU-29           [-1, 32, 16, 16]               0\n",
      "           Conv2d-30           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-31           [-1, 32, 16, 16]              64\n",
      "             ReLU-32           [-1, 32, 16, 16]               0\n",
      "    ResidualBlock-33           [-1, 32, 16, 16]               0\n",
      "           Conv2d-34             [-1, 64, 8, 8]          18,432\n",
      "      BatchNorm2d-35             [-1, 64, 8, 8]             128\n",
      "             ReLU-36             [-1, 64, 8, 8]               0\n",
      "           Conv2d-37             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-38             [-1, 64, 8, 8]             128\n",
      "           Conv2d-39             [-1, 64, 8, 8]          18,432\n",
      "      BatchNorm2d-40             [-1, 64, 8, 8]             128\n",
      "             ReLU-41             [-1, 64, 8, 8]               0\n",
      "    ResidualBlock-42             [-1, 64, 8, 8]               0\n",
      "           Conv2d-43             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-44             [-1, 64, 8, 8]             128\n",
      "             ReLU-45             [-1, 64, 8, 8]               0\n",
      "           Conv2d-46             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-47             [-1, 64, 8, 8]             128\n",
      "             ReLU-48             [-1, 64, 8, 8]               0\n",
      "    ResidualBlock-49             [-1, 64, 8, 8]               0\n",
      "        AvgPool2d-50             [-1, 64, 1, 1]               0\n",
      "           Linear-51                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 195,738\n",
      "Trainable params: 195,738\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.63\n",
      "Params size (MB): 0.75\n",
      "Estimated Total Size (MB): 4.38\n",
      "----------------------------------------------------------------\n",
      "ResNet(\n",
      "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n",
    "summary(net, input_size=(3,32,32))\n",
    "print (net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idB0-t3Mrsoo"
   },
   "source": [
    "## Learning rate and update strategy\n",
    "- 訓練 model時，若採用固定的 learning rate，容易找到 local minima 而非 global minima\n",
    "- Learning Rate Decay : 通常在訓練一定 epoch 後，會對學習率進行衰減，從而讓 model 收斂得更好，但不斷的縮小 learning rate 也有缺點(陷入saddle point)\n",
    "- 所以也有人使用 Cyclical Learning Rates: 設定學習率的上下限後，讓學習率在一定範圍內衰降或增加，在遇到 saddle point 不會卡住"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EgJejgUOVviV"
   },
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def adjust_learning_rate(optim, epoch):\n",
    "    for param_group in optimizer.param_groups:    # change the lr to what you define\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVaXImLHr5zR"
   },
   "source": [
    "### ReduceLROnPlateau\n",
    "- optimer : 指的是網絡的優化器\n",
    "- mode (str) : 可選擇'min'或者'max'，min表示當監控量停止下降的時候，學習率將減小，max表示當監控量停止上升的時候，學習率將減小。默認值為'min'\n",
    "- factor : 學習率每次降低多少，new_lr = old_lr * factor\n",
    "- patience=10 : 容忍網路的性能不提升的次數，高於這個次數就降低學習率\n",
    "- verbose（bool）: 如果為True，則為每次更新向stdout輸出一條消息。默認值：False\n",
    "- threshold（float）: 測量新最佳值的閾值，僅關注重大變化。默認值：1e-4\n",
    "- cooldown ： 減少lr後恢復正常操作之前要等待的時期數。默認值：0。\n",
    "- min_lr : 學習率的下限\n",
    "- eps : 適用於lr的最小衰減。如果新舊lr之間的差異小於eps，則忽略更新。默認值：1e-8。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet18\n",
    "- 用 Deep residual Network 來處理 degradation，這樣做能在網路層加深後，正確率至少不會變的更差。\n",
    "\n",
    "### Degradation\n",
    "- 當深度逐漸增加，我們發現56層的神經網路反而比20層網路結果還差。這樣的結果並非來自於 overfitting 和 Vanishing gradient problem，而是因為深度增加連帶著使得 training error 增加所導致的退化問題，以至於深層的特徵丟失了淺層特徵的原始模樣。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec00FGnr_SlR"
   },
   "outputs": [],
   "source": [
    "# 是否使用 pretrain model\n",
    "# net = models.resnet18(pretrained = True).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfBB9RKK6t8e",
    "outputId": "06239da4-94dc-4bfb-b535-2fbe7e3f8582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "learning rate:  0.05\n",
      "Train loss: 1.500 | Train acc: 0.444\n",
      "Val loss: 1.264 | Val acc: 0.532\n",
      "\n",
      "Epoch: 2\n",
      "learning rate:  0.05\n",
      "Train loss: 1.016 | Train acc: 0.634\n",
      "Val loss: 1.045 | Val acc: 0.625\n",
      "\n",
      "Epoch: 3\n",
      "learning rate:  0.05\n",
      "Train loss: 0.822 | Train acc: 0.709\n",
      "Val loss: 1.070 | Val acc: 0.637\n",
      "\n",
      "Epoch: 4\n",
      "learning rate:  0.05\n",
      "Train loss: 0.700 | Train acc: 0.756\n",
      "Val loss: 1.019 | Val acc: 0.659\n",
      "\n",
      "Epoch: 5\n",
      "learning rate:  0.05\n",
      "Train loss: 0.619 | Train acc: 0.785\n",
      "Val loss: 0.818 | Val acc: 0.714\n",
      "\n",
      "Epoch: 6\n",
      "learning rate:  0.05\n",
      "Train loss: 0.562 | Train acc: 0.804\n",
      "Val loss: 0.724 | Val acc: 0.747\n",
      "\n",
      "Epoch: 7\n",
      "learning rate:  0.05\n",
      "Train loss: 0.518 | Train acc: 0.820\n",
      "Val loss: 0.835 | Val acc: 0.718\n",
      "\n",
      "Epoch: 8\n",
      "learning rate:  0.05\n",
      "Train loss: 0.482 | Train acc: 0.833\n",
      "Val loss: 0.672 | Val acc: 0.764\n",
      "\n",
      "Epoch: 9\n",
      "learning rate:  0.05\n",
      "Train loss: 0.460 | Train acc: 0.839\n",
      "Val loss: 0.611 | Val acc: 0.787\n",
      "\n",
      "Epoch: 10\n",
      "learning rate:  0.025\n",
      "Train loss: 0.436 | Train acc: 0.848\n",
      "Val loss: 0.762 | Val acc: 0.757\n",
      "\n",
      "Epoch: 11\n",
      "learning rate:  0.025\n",
      "Train loss: 0.344 | Train acc: 0.881\n",
      "Val loss: 0.518 | Val acc: 0.825\n",
      "\n",
      "Epoch: 12\n",
      "learning rate:  0.025\n",
      "Train loss: 0.315 | Train acc: 0.891\n",
      "Val loss: 0.510 | Val acc: 0.832\n",
      "\n",
      "Epoch: 13\n",
      "learning rate:  0.025\n",
      "Train loss: 0.301 | Train acc: 0.895\n",
      "Val loss: 0.487 | Val acc: 0.833\n",
      "\n",
      "Epoch: 14\n",
      "learning rate:  0.025\n",
      "Train loss: 0.293 | Train acc: 0.900\n",
      "Val loss: 0.753 | Val acc: 0.785\n",
      "\n",
      "Epoch: 15\n",
      "learning rate:  0.025\n",
      "Train loss: 0.287 | Train acc: 0.899\n",
      "Val loss: 0.514 | Val acc: 0.821\n",
      "\n",
      "Epoch: 16\n",
      "learning rate:  0.025\n",
      "Train loss: 0.280 | Train acc: 0.902\n",
      "Val loss: 0.579 | Val acc: 0.818\n",
      "\n",
      "Epoch: 17\n",
      "learning rate:  0.025\n",
      "Train loss: 0.276 | Train acc: 0.904\n",
      "Val loss: 0.549 | Val acc: 0.822\n",
      "\n",
      "Epoch: 18\n",
      "learning rate:  0.025\n",
      "Train loss: 0.267 | Train acc: 0.908\n",
      "Val loss: 0.575 | Val acc: 0.811\n",
      "\n",
      "Epoch: 19\n",
      "learning rate:  0.025\n",
      "Train loss: 0.260 | Train acc: 0.910\n",
      "Val loss: 0.546 | Val acc: 0.819\n",
      "\n",
      "Epoch: 20\n",
      "learning rate:  0.0125\n",
      "Train loss: 0.254 | Train acc: 0.910\n",
      "Val loss: 0.519 | Val acc: 0.822\n",
      "\n",
      "Epoch: 21\n",
      "learning rate:  0.0125\n",
      "Train loss: 0.176 | Train acc: 0.941\n",
      "Val loss: 0.434 | Val acc: 0.857\n",
      "\n",
      "Epoch: 22\n",
      "learning rate:  0.0125\n",
      "Train loss: 0.150 | Train acc: 0.951\n",
      "Val loss: 0.488 | Val acc: 0.847\n",
      "\n",
      "Epoch: 23\n",
      "learning rate:  0.0125\n",
      "Train loss: 0.134 | Train acc: 0.958\n",
      "Val loss: 0.446 | Val acc: 0.851\n",
      "\n",
      "Epoch: 24\n",
      "learning rate:  0.0125\n",
      "Train loss: 0.132 | Train acc: 0.958\n",
      "Val loss: 0.473 | Val acc: 0.850\n",
      "\n",
      "Epoch: 25\n",
      "learning rate:  0.0125\n",
      "Train loss: 0.126 | Train acc: 0.960\n",
      "Val loss: 0.599 | Val acc: 0.829\n",
      "\n",
      "Epoch: 26\n",
      "learning rate:  0.0125\n",
      "Train loss: 0.122 | Train acc: 0.960\n",
      "Val loss: 0.523 | Val acc: 0.836\n",
      "\n",
      "Epoch: 27\n",
      "learning rate:  0.0125\n",
      "Train loss: 0.119 | Train acc: 0.962\n",
      "Val loss: 0.538 | Val acc: 0.841\n",
      "\n",
      "Epoch: 28\n",
      "learning rate:  0.0125\n",
      "Train loss: 0.117 | Train acc: 0.961\n",
      "Val loss: 0.556 | Val acc: 0.840\n",
      "\n",
      "Epoch: 29\n",
      "learning rate:  0.0125\n",
      "Train loss: 0.109 | Train acc: 0.965\n",
      "Val loss: 0.611 | Val acc: 0.823\n",
      "\n",
      "Epoch: 30\n",
      "learning rate:  0.00625\n",
      "Train loss: 0.112 | Train acc: 0.962\n",
      "Val loss: 0.560 | Val acc: 0.833\n",
      "\n",
      "Epoch: 31\n",
      "learning rate:  0.00625\n",
      "Train loss: 0.073 | Train acc: 0.980\n",
      "Val loss: 0.476 | Val acc: 0.857\n",
      "\n",
      "Epoch: 32\n",
      "learning rate:  0.00625\n",
      "Train loss: 0.050 | Train acc: 0.989\n",
      "Val loss: 0.479 | Val acc: 0.856\n",
      "\n",
      "Epoch: 33\n",
      "learning rate:  0.00625\n",
      "Train loss: 0.043 | Train acc: 0.991\n",
      "Val loss: 0.473 | Val acc: 0.860\n",
      "\n",
      "Epoch: 34\n",
      "learning rate:  0.00625\n",
      "Train loss: 0.037 | Train acc: 0.993\n",
      "Val loss: 0.486 | Val acc: 0.859\n",
      "\n",
      "Epoch: 35\n",
      "learning rate:  0.00625\n",
      "Train loss: 0.035 | Train acc: 0.994\n",
      "Val loss: 0.506 | Val acc: 0.857\n",
      "\n",
      "Epoch: 36\n",
      "learning rate:  0.00625\n",
      "Train loss: 0.030 | Train acc: 0.995\n",
      "Val loss: 0.495 | Val acc: 0.862\n",
      "\n",
      "Epoch: 37\n",
      "learning rate:  0.00625\n",
      "Train loss: 0.027 | Train acc: 0.997\n",
      "Val loss: 0.498 | Val acc: 0.859\n",
      "\n",
      "Epoch: 38\n",
      "learning rate:  0.00625\n",
      "Train loss: 0.025 | Train acc: 0.997\n",
      "Val loss: 0.514 | Val acc: 0.858\n",
      "\n",
      "Epoch: 39\n",
      "learning rate:  0.00625\n",
      "Train loss: 0.024 | Train acc: 0.997\n",
      "Val loss: 0.511 | Val acc: 0.856\n",
      "\n",
      "Epoch: 40\n",
      "learning rate:  0.003125\n",
      "Train loss: 0.021 | Train acc: 0.998\n",
      "Val loss: 0.524 | Val acc: 0.857\n",
      "\n",
      "Epoch: 41\n",
      "learning rate:  0.003125\n",
      "Train loss: 0.017 | Train acc: 0.999\n",
      "Val loss: 0.508 | Val acc: 0.854\n",
      "\n",
      "Epoch: 42\n",
      "learning rate:  0.003125\n",
      "Train loss: 0.015 | Train acc: 0.999\n",
      "Val loss: 0.508 | Val acc: 0.861\n",
      "\n",
      "Epoch: 43\n",
      "learning rate:  0.003125\n",
      "Train loss: 0.013 | Train acc: 1.000\n",
      "Val loss: 0.498 | Val acc: 0.862\n",
      "\n",
      "Epoch: 44\n",
      "learning rate:  0.003125\n",
      "Train loss: 0.013 | Train acc: 1.000\n",
      "Val loss: 0.505 | Val acc: 0.860\n",
      "\n",
      "Epoch: 45\n",
      "learning rate:  0.003125\n",
      "Train loss: 0.013 | Train acc: 1.000\n",
      "Val loss: 0.504 | Val acc: 0.860\n",
      "\n",
      "Epoch: 46\n",
      "learning rate:  0.003125\n",
      "Train loss: 0.012 | Train acc: 1.000\n",
      "Val loss: 0.515 | Val acc: 0.858\n",
      "\n",
      "Epoch: 47\n",
      "learning rate:  0.003125\n",
      "Train loss: 0.012 | Train acc: 1.000\n",
      "Val loss: 0.489 | Val acc: 0.864\n",
      "\n",
      "Epoch: 48\n",
      "learning rate:  0.003125\n",
      "Train loss: 0.011 | Train acc: 1.000\n",
      "Val loss: 0.516 | Val acc: 0.857\n",
      "\n",
      "Epoch: 49\n",
      "learning rate:  0.003125\n",
      "Train loss: 0.011 | Train acc: 1.000\n",
      "Val loss: 0.509 | Val acc: 0.863\n",
      "\n",
      "Epoch: 50\n",
      "learning rate:  0.0015625\n",
      "Train loss: 0.011 | Train acc: 1.000\n",
      "Val loss: 0.505 | Val acc: 0.860\n",
      "\n",
      "Epoch: 51\n",
      "learning rate:  0.0015625\n",
      "Train loss: 0.010 | Train acc: 1.000\n",
      "Val loss: 0.490 | Val acc: 0.863\n",
      "\n",
      "Epoch: 52\n",
      "learning rate:  0.0015625\n",
      "Train loss: 0.010 | Train acc: 1.000\n",
      "Val loss: 0.508 | Val acc: 0.861\n",
      "\n",
      "Epoch: 53\n",
      "learning rate:  0.0015625\n",
      "Train loss: 0.010 | Train acc: 1.000\n",
      "Val loss: 0.519 | Val acc: 0.861\n",
      "\n",
      "Epoch: 54\n",
      "learning rate:  0.0015625\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.501 | Val acc: 0.861\n",
      "\n",
      "Epoch: 55\n",
      "learning rate:  0.0015625\n",
      "Train loss: 0.010 | Train acc: 1.000\n",
      "Val loss: 0.504 | Val acc: 0.862\n",
      "\n",
      "Epoch: 56\n",
      "learning rate:  0.0015625\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.499 | Val acc: 0.862\n",
      "\n",
      "Epoch: 57\n",
      "learning rate:  0.0015625\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.503 | Val acc: 0.859\n",
      "\n",
      "Epoch: 58\n",
      "learning rate:  0.0015625\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.497 | Val acc: 0.859\n",
      "\n",
      "Epoch: 59\n",
      "learning rate:  0.0015625\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.491 | Val acc: 0.866\n",
      "\n",
      "Epoch: 60\n",
      "learning rate:  0.00078125\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.525 | Val acc: 0.857\n",
      "\n",
      "Epoch: 61\n",
      "learning rate:  0.00078125\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.510 | Val acc: 0.858\n",
      "\n",
      "Epoch: 62\n",
      "learning rate:  0.00078125\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.521 | Val acc: 0.856\n",
      "\n",
      "Epoch: 63\n",
      "learning rate:  0.00078125\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.504 | Val acc: 0.858\n",
      "\n",
      "Epoch: 64\n",
      "learning rate:  0.00078125\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.505 | Val acc: 0.863\n",
      "\n",
      "Epoch: 65\n",
      "learning rate:  0.00078125\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.498 | Val acc: 0.859\n",
      "\n",
      "Epoch: 66\n",
      "learning rate:  0.00078125\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.507 | Val acc: 0.861\n",
      "\n",
      "Epoch: 67\n",
      "learning rate:  0.00078125\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.502 | Val acc: 0.857\n",
      "\n",
      "Epoch: 68\n",
      "learning rate:  0.00078125\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.501 | Val acc: 0.860\n",
      "\n",
      "Epoch: 69\n",
      "learning rate:  0.00078125\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.510 | Val acc: 0.861\n",
      "\n",
      "Epoch: 70\n",
      "learning rate:  0.000390625\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.507 | Val acc: 0.861\n",
      "\n",
      "Epoch: 71\n",
      "learning rate:  0.000390625\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.505 | Val acc: 0.863\n",
      "\n",
      "Epoch: 72\n",
      "learning rate:  0.000390625\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.519 | Val acc: 0.857\n",
      "\n",
      "Epoch: 73\n",
      "learning rate:  0.000390625\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.510 | Val acc: 0.860\n",
      "\n",
      "Epoch: 74\n",
      "learning rate:  0.000390625\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.480 | Val acc: 0.867\n",
      "\n",
      "Epoch: 75\n",
      "learning rate:  0.000390625\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.507 | Val acc: 0.860\n",
      "\n",
      "Epoch: 76\n",
      "learning rate:  0.000390625\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.496 | Val acc: 0.860\n",
      "\n",
      "Epoch: 77\n",
      "learning rate:  0.000390625\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.495 | Val acc: 0.862\n",
      "\n",
      "Epoch: 78\n",
      "learning rate:  0.000390625\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.511 | Val acc: 0.859\n",
      "\n",
      "Epoch: 79\n",
      "learning rate:  0.000390625\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.501 | Val acc: 0.860\n",
      "\n",
      "Epoch: 80\n",
      "learning rate:  0.0001953125\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.506 | Val acc: 0.864\n",
      "\n",
      "Epoch: 81\n",
      "learning rate:  0.0001953125\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.494 | Val acc: 0.865\n",
      "\n",
      "Epoch: 82\n",
      "learning rate:  0.0001953125\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.508 | Val acc: 0.858\n",
      "\n",
      "Epoch: 83\n",
      "learning rate:  0.0001953125\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.502 | Val acc: 0.860\n",
      "\n",
      "Epoch: 84\n",
      "learning rate:  0.0001953125\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.501 | Val acc: 0.860\n",
      "\n",
      "Epoch: 85\n",
      "learning rate:  0.0001953125\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.508 | Val acc: 0.859\n",
      "\n",
      "Epoch: 86\n",
      "learning rate:  0.0001953125\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.520 | Val acc: 0.858\n",
      "\n",
      "Epoch: 87\n",
      "learning rate:  0.0001953125\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.510 | Val acc: 0.861\n",
      "\n",
      "Epoch: 88\n",
      "learning rate:  0.0001953125\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.497 | Val acc: 0.861\n",
      "\n",
      "Epoch: 89\n",
      "learning rate:  0.0001953125\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.509 | Val acc: 0.860\n",
      "\n",
      "Epoch: 90\n",
      "learning rate:  9.765625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.505 | Val acc: 0.857\n",
      "\n",
      "Epoch: 91\n",
      "learning rate:  9.765625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.517 | Val acc: 0.858\n",
      "\n",
      "Epoch: 92\n",
      "learning rate:  9.765625e-05\n",
      "Train loss: 0.009 | Train acc: 1.000\n",
      "Val loss: 0.497 | Val acc: 0.860\n",
      "\n",
      "Epoch: 93\n",
      "learning rate:  9.765625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.508 | Val acc: 0.860\n",
      "\n",
      "Epoch: 94\n",
      "learning rate:  9.765625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.513 | Val acc: 0.858\n",
      "\n",
      "Epoch: 95\n",
      "learning rate:  9.765625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.514 | Val acc: 0.858\n",
      "\n",
      "Epoch: 96\n",
      "learning rate:  9.765625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.512 | Val acc: 0.858\n",
      "\n",
      "Epoch: 97\n",
      "learning rate:  9.765625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.525 | Val acc: 0.861\n",
      "\n",
      "Epoch: 98\n",
      "learning rate:  9.765625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.515 | Val acc: 0.858\n",
      "\n",
      "Epoch: 99\n",
      "learning rate:  9.765625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.515 | Val acc: 0.860\n",
      "\n",
      "Epoch: 100\n",
      "learning rate:  4.8828125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.499 | Val acc: 0.861\n",
      "\n",
      "Epoch: 101\n",
      "learning rate:  4.8828125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.504 | Val acc: 0.858\n",
      "\n",
      "Epoch: 102\n",
      "learning rate:  4.8828125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.531 | Val acc: 0.853\n",
      "\n",
      "Epoch: 103\n",
      "learning rate:  4.8828125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.518 | Val acc: 0.859\n",
      "\n",
      "Epoch: 104\n",
      "learning rate:  4.8828125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.513 | Val acc: 0.860\n",
      "\n",
      "Epoch: 105\n",
      "learning rate:  4.8828125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.515 | Val acc: 0.861\n",
      "\n",
      "Epoch: 106\n",
      "learning rate:  4.8828125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.519 | Val acc: 0.856\n",
      "\n",
      "Epoch: 107\n",
      "learning rate:  4.8828125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.508 | Val acc: 0.859\n",
      "\n",
      "Epoch: 108\n",
      "learning rate:  4.8828125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.511 | Val acc: 0.862\n",
      "\n",
      "Epoch: 109\n",
      "learning rate:  4.8828125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.511 | Val acc: 0.861\n",
      "\n",
      "Epoch: 110\n",
      "learning rate:  2.44140625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.521 | Val acc: 0.859\n",
      "\n",
      "Epoch: 111\n",
      "learning rate:  2.44140625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.505 | Val acc: 0.860\n",
      "\n",
      "Epoch: 112\n",
      "learning rate:  2.44140625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.498 | Val acc: 0.857\n",
      "\n",
      "Epoch: 113\n",
      "learning rate:  2.44140625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.501 | Val acc: 0.863\n",
      "\n",
      "Epoch: 114\n",
      "learning rate:  2.44140625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.517 | Val acc: 0.859\n",
      "\n",
      "Epoch: 115\n",
      "learning rate:  2.44140625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.522 | Val acc: 0.856\n",
      "\n",
      "Epoch: 116\n",
      "learning rate:  2.44140625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.511 | Val acc: 0.862\n",
      "\n",
      "Epoch: 117\n",
      "learning rate:  2.44140625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.506 | Val acc: 0.860\n",
      "\n",
      "Epoch: 118\n",
      "learning rate:  2.44140625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.509 | Val acc: 0.863\n",
      "\n",
      "Epoch: 119\n",
      "learning rate:  2.44140625e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.507 | Val acc: 0.861\n",
      "\n",
      "Epoch: 120\n",
      "learning rate:  1.220703125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.521 | Val acc: 0.857\n",
      "\n",
      "Epoch: 121\n",
      "learning rate:  1.220703125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.520 | Val acc: 0.859\n",
      "\n",
      "Epoch: 122\n",
      "learning rate:  1.220703125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.517 | Val acc: 0.860\n",
      "\n",
      "Epoch: 123\n",
      "learning rate:  1.220703125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.518 | Val acc: 0.857\n",
      "\n",
      "Epoch: 124\n",
      "learning rate:  1.220703125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.503 | Val acc: 0.860\n",
      "\n",
      "Epoch: 125\n",
      "learning rate:  1.220703125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.508 | Val acc: 0.859\n",
      "\n",
      "Epoch: 126\n",
      "learning rate:  1.220703125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.506 | Val acc: 0.860\n",
      "\n",
      "Epoch: 127\n",
      "learning rate:  1.220703125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.505 | Val acc: 0.860\n",
      "\n",
      "Epoch: 128\n",
      "learning rate:  1.220703125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.515 | Val acc: 0.860\n",
      "\n",
      "Epoch: 129\n",
      "learning rate:  1.220703125e-05\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.521 | Val acc: 0.857\n",
      "\n",
      "Epoch: 130\n",
      "learning rate:  6.103515625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.498 | Val acc: 0.860\n",
      "\n",
      "Epoch: 131\n",
      "learning rate:  6.103515625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.516 | Val acc: 0.860\n",
      "\n",
      "Epoch: 132\n",
      "learning rate:  6.103515625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.526 | Val acc: 0.855\n",
      "\n",
      "Epoch: 133\n",
      "learning rate:  6.103515625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.519 | Val acc: 0.859\n",
      "\n",
      "Epoch: 134\n",
      "learning rate:  6.103515625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.516 | Val acc: 0.856\n",
      "\n",
      "Epoch: 135\n",
      "learning rate:  6.103515625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.527 | Val acc: 0.859\n",
      "\n",
      "Epoch: 136\n",
      "learning rate:  6.103515625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.512 | Val acc: 0.860\n",
      "\n",
      "Epoch: 137\n",
      "learning rate:  6.103515625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.508 | Val acc: 0.856\n",
      "\n",
      "Epoch: 138\n",
      "learning rate:  6.103515625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.506 | Val acc: 0.859\n",
      "\n",
      "Epoch: 139\n",
      "learning rate:  6.103515625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.508 | Val acc: 0.865\n",
      "\n",
      "Epoch: 140\n",
      "learning rate:  3.0517578125e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.517 | Val acc: 0.860\n",
      "\n",
      "Epoch: 141\n",
      "learning rate:  3.0517578125e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.501 | Val acc: 0.861\n",
      "\n",
      "Epoch: 142\n",
      "learning rate:  3.0517578125e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.510 | Val acc: 0.862\n",
      "\n",
      "Epoch: 143\n",
      "learning rate:  3.0517578125e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.522 | Val acc: 0.858\n",
      "\n",
      "Epoch: 144\n",
      "learning rate:  3.0517578125e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.516 | Val acc: 0.859\n",
      "\n",
      "Epoch: 145\n",
      "learning rate:  3.0517578125e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.510 | Val acc: 0.861\n",
      "\n",
      "Epoch: 146\n",
      "learning rate:  3.0517578125e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.504 | Val acc: 0.860\n",
      "\n",
      "Epoch: 147\n",
      "learning rate:  3.0517578125e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.513 | Val acc: 0.860\n",
      "\n",
      "Epoch: 148\n",
      "learning rate:  3.0517578125e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.509 | Val acc: 0.862\n",
      "\n",
      "Epoch: 149\n",
      "learning rate:  3.0517578125e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.496 | Val acc: 0.862\n",
      "\n",
      "Epoch: 150\n",
      "learning rate:  1.52587890625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.506 | Val acc: 0.860\n",
      "\n",
      "Epoch: 151\n",
      "learning rate:  1.52587890625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.506 | Val acc: 0.862\n",
      "\n",
      "Epoch: 152\n",
      "learning rate:  1.52587890625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.507 | Val acc: 0.860\n",
      "\n",
      "Epoch: 153\n",
      "learning rate:  1.52587890625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.509 | Val acc: 0.856\n",
      "\n",
      "Epoch: 154\n",
      "learning rate:  1.52587890625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.505 | Val acc: 0.863\n",
      "\n",
      "Epoch: 155\n",
      "learning rate:  1.52587890625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.501 | Val acc: 0.863\n",
      "\n",
      "Epoch: 156\n",
      "learning rate:  1.52587890625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.516 | Val acc: 0.863\n",
      "\n",
      "Epoch: 157\n",
      "learning rate:  1.52587890625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.510 | Val acc: 0.859\n",
      "\n",
      "Epoch: 158\n",
      "learning rate:  1.52587890625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.521 | Val acc: 0.860\n",
      "\n",
      "Epoch: 159\n",
      "learning rate:  1.52587890625e-06\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.512 | Val acc: 0.858\n",
      "\n",
      "Epoch: 160\n",
      "learning rate:  7.62939453125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.505 | Val acc: 0.858\n",
      "\n",
      "Epoch: 161\n",
      "learning rate:  7.62939453125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.523 | Val acc: 0.855\n",
      "\n",
      "Epoch: 162\n",
      "learning rate:  7.62939453125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.509 | Val acc: 0.859\n",
      "\n",
      "Epoch: 163\n",
      "learning rate:  7.62939453125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.512 | Val acc: 0.859\n",
      "\n",
      "Epoch: 164\n",
      "learning rate:  7.62939453125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.508 | Val acc: 0.862\n",
      "\n",
      "Epoch: 165\n",
      "learning rate:  7.62939453125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.501 | Val acc: 0.861\n",
      "\n",
      "Epoch: 166\n",
      "learning rate:  7.62939453125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.505 | Val acc: 0.863\n",
      "\n",
      "Epoch: 167\n",
      "learning rate:  7.62939453125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.499 | Val acc: 0.858\n",
      "\n",
      "Epoch: 168\n",
      "learning rate:  7.62939453125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.509 | Val acc: 0.861\n",
      "\n",
      "Epoch: 169\n",
      "learning rate:  7.62939453125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.509 | Val acc: 0.860\n",
      "\n",
      "Epoch: 170\n",
      "learning rate:  3.814697265625e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.509 | Val acc: 0.862\n",
      "\n",
      "Epoch: 171\n",
      "learning rate:  3.814697265625e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.511 | Val acc: 0.861\n",
      "\n",
      "Epoch: 172\n",
      "learning rate:  3.814697265625e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.513 | Val acc: 0.859\n",
      "\n",
      "Epoch: 173\n",
      "learning rate:  3.814697265625e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.489 | Val acc: 0.864\n",
      "\n",
      "Epoch: 174\n",
      "learning rate:  3.814697265625e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.503 | Val acc: 0.861\n",
      "\n",
      "Epoch: 175\n",
      "learning rate:  3.814697265625e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.521 | Val acc: 0.857\n",
      "\n",
      "Epoch: 176\n",
      "learning rate:  3.814697265625e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.489 | Val acc: 0.865\n",
      "\n",
      "Epoch: 177\n",
      "learning rate:  3.814697265625e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.500 | Val acc: 0.864\n",
      "\n",
      "Epoch: 178\n",
      "learning rate:  3.814697265625e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.500 | Val acc: 0.863\n",
      "\n",
      "Epoch: 179\n",
      "learning rate:  3.814697265625e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.535 | Val acc: 0.856\n",
      "\n",
      "Epoch: 180\n",
      "learning rate:  1.9073486328125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.524 | Val acc: 0.857\n",
      "\n",
      "Epoch: 181\n",
      "learning rate:  1.9073486328125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.514 | Val acc: 0.860\n",
      "\n",
      "Epoch: 182\n",
      "learning rate:  1.9073486328125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.511 | Val acc: 0.859\n",
      "\n",
      "Epoch: 183\n",
      "learning rate:  1.9073486328125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.502 | Val acc: 0.859\n",
      "\n",
      "Epoch: 184\n",
      "learning rate:  1.9073486328125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.503 | Val acc: 0.864\n",
      "\n",
      "Epoch: 185\n",
      "learning rate:  1.9073486328125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.502 | Val acc: 0.865\n",
      "\n",
      "Epoch: 186\n",
      "learning rate:  1.9073486328125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.523 | Val acc: 0.856\n",
      "\n",
      "Epoch: 187\n",
      "learning rate:  1.9073486328125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.510 | Val acc: 0.857\n",
      "\n",
      "Epoch: 188\n",
      "learning rate:  1.9073486328125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.520 | Val acc: 0.859\n",
      "\n",
      "Epoch: 189\n",
      "learning rate:  1.9073486328125e-07\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.517 | Val acc: 0.861\n",
      "\n",
      "Epoch: 190\n",
      "learning rate:  9.5367431640625e-08\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.500 | Val acc: 0.860\n",
      "\n",
      "Epoch: 191\n",
      "learning rate:  9.5367431640625e-08\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.512 | Val acc: 0.860\n",
      "\n",
      "Epoch: 192\n",
      "learning rate:  9.5367431640625e-08\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.516 | Val acc: 0.857\n",
      "\n",
      "Epoch: 193\n",
      "learning rate:  9.5367431640625e-08\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.509 | Val acc: 0.860\n",
      "\n",
      "Epoch: 194\n",
      "learning rate:  9.5367431640625e-08\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.519 | Val acc: 0.861\n",
      "\n",
      "Epoch: 195\n",
      "learning rate:  9.5367431640625e-08\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.497 | Val acc: 0.865\n",
      "\n",
      "Epoch: 196\n",
      "learning rate:  9.5367431640625e-08\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.525 | Val acc: 0.863\n",
      "\n",
      "Epoch: 197\n",
      "learning rate:  9.5367431640625e-08\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.503 | Val acc: 0.857\n",
      "\n",
      "Epoch: 198\n",
      "learning rate:  9.5367431640625e-08\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.514 | Val acc: 0.860\n",
      "\n",
      "Epoch: 199\n",
      "learning rate:  9.5367431640625e-08\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.510 | Val acc: 0.860\n",
      "\n",
      "Epoch: 200\n",
      "learning rate:  4.76837158203125e-08\n",
      "Train loss: 0.008 | Train acc: 1.000\n",
      "Val loss: 0.521 | Val acc: 0.858\n",
      "Test loss: 0.531 | Test acc: 0.862\n"
     ]
    }
   ],
   "source": [
    "# 定義損失函數和優化方式\n",
    "criterion = nn.CrossEntropyLoss()  #loss function \n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)   # momentum-SGD，採用L2正則化（權重衰減）\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.05, patience=2, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_loss, train_acc = [], []\n",
    "    val_loss, val_acc = [], []\n",
    "    for epoch in range(EPOCH):\n",
    "        # train\n",
    "        net.train()\n",
    "        sum1_loss, sum2_loss = 0.0, 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        print('\\nEpoch: %d' % (epoch + 1))\n",
    "        for i, traindata in enumerate(trainloader, 0):\n",
    "            # prepare data\n",
    "            inputs, train_labels = traindata\n",
    "            inputs, train_labels = inputs.to(device), train_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward\n",
    "            train_outputs = net(inputs)\n",
    "            trainloss = criterion(train_outputs, train_labels)\n",
    "            trainloss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 每訓練1個batch的loss和acc\n",
    "            sum1_loss += trainloss.item()\n",
    "            _, predicted = torch.max(train_outputs.data, 1)     # 取得分數最高的那個類 (outputs.data的index)\n",
    "            total += train_labels.size(0)\n",
    "            correct += predicted.eq(train_labels.data).cpu().sum()\n",
    "\n",
    "            # learning rate schedule\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            lr /= 2  \n",
    "            adjust_learning_rate(optim, epoch)\n",
    "        print(\"learning rate: \",  optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        loss1 = sum1_loss / (i + 1)\n",
    "        acc1 = correct / total\n",
    "        print(\"Train loss: %.3f | Train acc: %.3f\" % (loss1, acc1))\n",
    "        train_loss.append(loss1)\n",
    "        train_acc.append(acc1.item())\n",
    "\n",
    "        # 用val驗證\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for j, valdata in enumerate(valloader, 0):\n",
    "                net.eval()\n",
    "                images, val_labels = valdata\n",
    "                images, val_labels = images.to(device), val_labels.to(device)\n",
    "                val_outputs = net(images)\n",
    "                valloss = criterion(val_outputs, val_labels)\n",
    "                        \n",
    "                sum2_loss += valloss.item()\n",
    "                _, predicted = torch.max(val_outputs.data, 1)\n",
    "                total += val_labels.size(0)\n",
    "                correct += (predicted == val_labels).sum()\n",
    "                    \n",
    "            loss2 = sum2_loss / (j + 1)\n",
    "            acc2 = correct / total\n",
    "            print(\"Val loss: %.3f | Val acc: %.3f\" % (loss2, acc2))\n",
    "            val_loss.append(loss2)\n",
    "            val_acc.append(acc2.item())\n",
    "\n",
    "    # 用test測試\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        sum3_loss = 0.0\n",
    "        for k, testdata in enumerate(testloader, 0):\n",
    "            net.eval()\n",
    "            imgs, test_labels = testdata\n",
    "            imgs, test_labels = imgs.to(device), test_labels.to(device)\n",
    "            test_outputs = net(imgs)\n",
    "            testloss = criterion(test_outputs, test_labels)\n",
    "                        \n",
    "            sum3_loss += testloss.item()\n",
    "            _, predicted = torch.max(test_outputs.data, 1)\n",
    "            total += test_labels.size(0)\n",
    "            correct += (predicted == test_labels).sum()\n",
    "                    \n",
    "        loss3 = sum3_loss / (k + 1)\n",
    "        acc3 = correct / total\n",
    "        print(\"Test loss: %.3f | Test acc: %.3f\" % (loss3, acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "g-QShVIgmicX",
    "outputId": "964baac6-a309-4a11-a091-23453babdf33"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot accuracy curve\n",
    "plt.figure(0)\n",
    "plt.plot(range(1,EPOCH+1,1), np.array(train_acc), 'r-', label= \"train accuracy\") \n",
    "plt.plot(range(1,EPOCH+1,1), np.array(val_acc), 'b-', label= \"val accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot loss curve\n",
    "plt.figure(1)\n",
    "plt.plot(range(1,EPOCH+1,1), np.array(train_loss), 'r-', label= \"train loss\") \n",
    "plt.plot(range(1,EPOCH+1,1), np.array(val_loss), 'b-', label= \"val loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7ZyqlysPhfi"
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/greatcodes/pytorch-cnn-resnet18-cifar10"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "184435fbcabc4db88b069bc1b5d5188b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24ccea8108d74cf3a0e28e79731e28f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "667dd381dd9f48df951ac521eb3e7a3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8b37ed735d54c689dd37c32e68cb3d1",
      "placeholder": "​",
      "style": "IPY_MODEL_9b9c2d61e8664e2eb945d79481e199e5",
      "value": "100%"
     }
    },
    "70e8542b6c6b4526bd904077c7693b2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_667dd381dd9f48df951ac521eb3e7a3e",
       "IPY_MODEL_d9c4be8524af4e09aa38fde9842bf2b4",
       "IPY_MODEL_f937b15f2da944e483e1fa58b28526d3"
      ],
      "layout": "IPY_MODEL_184435fbcabc4db88b069bc1b5d5188b"
     }
    },
    "7d07480af9504349bd46804fe5df7020": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "89505cba3db942b6ae37a34138c1d614": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91152223b154456888596988b459022a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9b9c2d61e8664e2eb945d79481e199e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8b37ed735d54c689dd37c32e68cb3d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9c4be8524af4e09aa38fde9842bf2b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24ccea8108d74cf3a0e28e79731e28f6",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7d07480af9504349bd46804fe5df7020",
      "value": 170498071
     }
    },
    "f937b15f2da944e483e1fa58b28526d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89505cba3db942b6ae37a34138c1d614",
      "placeholder": "​",
      "style": "IPY_MODEL_91152223b154456888596988b459022a",
      "value": " 170498071/170498071 [00:02&lt;00:00, 55971757.18it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
