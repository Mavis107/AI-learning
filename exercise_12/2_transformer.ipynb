{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026e0255",
   "metadata": {},
   "source": [
    "# Transformers - Attention is All You Need\n",
    "Welcome to the captivating world of Transformers in Natural Language Processing (NLP)! In recent years, Transformers have emerged as a groundbreaking paradigm in NLP, revolutionizing how we comprehend and process language. Unlike older architectures such as LSTMs (Long Short-Term Memory) and CNNs (Convolutional Neural Networks), Transformers leverage self-attention mechanisms, enabling them to capture long-range dependencies more effectively and allowing for parallelization of computations. The seminal paper ['Attention Is All You Need'](https://arxiv.org/abs/1706.03762) introduced the Transformer model, showcasing its prowess in various language tasks and laying the foundation for its widespread adoption. This Jupyter notebook is designed to unravel the intricacies of Transformers, highlighting their architectural nuances, contrasting them with traditional models, and demonstrating their applications across diverse NLP domains! (And in case you were wondering - yes, a certain transformer model created this text :D)\n",
    "\n",
    "We will stick very closely to the implementation presented in the paper to build our own translation model! There is no need to read it though, you can find the necessary sections of the paper before each task, together with some explanations. If you did not work on exercise 11, we would really advise you to do that first, since we will pick up on many of those topics!\n",
    "\n",
    "Alright - let's do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3c01bc",
   "metadata": {},
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e6fa9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\nimport os\\n\\ngdrive_path='/content/gdrive/MyDrive/i2dl/exercise_11'\\n\\n# This will mount your google drive under 'MyDrive'\\ndrive.mount('/content/gdrive', force_remount=True)\\n# In order to access the files in this notebook we have to navigate to the correct folder\\nos.chdir(gdrive_path)\\n# Check manually if all files are present\\nprint(sorted(os.listdir()))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_11) is given.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_11'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc9819",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f568e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.util import notebook_util as util\n",
    "from exercise_code.network import *\n",
    "from exercise_code.tests import *\n",
    "from exercise_code.Trainer import MPS_AVAILABLE\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "root_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "model_path = os.path.join(os.getcwd(), 'models')\n",
    "submission_path = os.path.join(os.getcwd(), 'submission_files')\n",
    "pretrained_model_path = os.path.join(model_path, 'pretrainedModels')\n",
    "dataset_path = os.path.join(root_path, 'datasets', 'transformerDatasets')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a27adc",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "<!-- <img src=\"images/Transformer-Transformer.drawio.png\" width=\"2500\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Transformer.drawio.png\"  width=\"2500\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The model consists of two bigger building blocks, the encoder and the decoder. The encoder processes the input to our model. In our translation model, this input would be the sentence in the source language. The decoder part iteratively produces an output sequence. As an input it takes the already predicted words and given that sequence and the encoder input, it produces an output sequence as follows:\n",
    "\n",
    "| Encoder Input                       | Decoder Input                                      | Decoder Output                                   |\n",
    "|-------------------------------------|----------------------------------------------------|--------------------------------------------------|\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\"]                                       | [\"Hallo\"]                                        |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\"]                              | [\"Hallo\", \"wie\"]                                 |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\", \"wie\"]                       | [\"Hallo\", \"wie\", \"geht's\"]                       |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\", \"wie\", \"geht's\"]             | [\"Hallo\", \"wie\", \"geht's\", \"dir\"]                |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\", \"wie\", \"geht's\", \"dir\"]      | [\"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\"]           |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\"] | [\"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\", \"\\<end>\"] |\n",
    "\n",
    "And the predicted end token breaks the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310bd92",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>WARNING: Task Implementation</h3>\n",
    "    <p>Throughout this notebook you will as always have to complete several tasks to complete the individual modules! Please be aware though, that some Modules may have several tasks that have to be completed, but please <strong>only</strong> concentrate on the <strong>current task</strong> and the <strong>corresponding hints</strong> (if there are any;). In other words, if you are working on Task 1, and there is also Task 4 in the TODOs of that Module, you don't have to work on it at this moment! <br>\n",
    "    Also, if we mention any specific pytorch modules in the task description or hints, you <strong>are allowed to use them!</strong> With that said, let's work on your first task!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d0ecf3b6214a5",
   "metadata": {},
   "source": [
    "# Cross Attention vs Self Attention\n",
    "\n",
    "You might have noticed, that we are always talking about inputs to an attention mechanism and the context we want to compare it to. If we use the terminology from the paper, our inputs form the query of our attention mechanism, and the context form the key - value pairs! \n",
    "\n",
    "## Cross Attention\n",
    "Cross-Attention might be the easier idea to understand. We use two different sources for the input and context! In the transformer model, this type of attention will be used in the decoder, that way we can contextualize our output with the actual input to our model. \n",
    "\n",
    "Let's say you ask the model a question like \"Hello, how are you?\". This will be processed by the encoder and it will give us some output. Next, the decoder will start to produce its output token by token, and at each step, the output should obviously depend on our initial question! We can achieve this by using cross attention, where the inputs to the mechanism come from the decoder itself, and the context comes from the encoder. It is trying to give context from our input to its output - something we definetly want to have!\n",
    "\n",
    "## Self Attention\n",
    "In Self-Attention, the input and the context source are the same! That means, every word in a sequence is attending to all words in the same sequence, or in other words - the sentence is attending to itself. This form is used in both the encoder and decoder, and is used to process their inputs to gain a \"deeper\" understanding of the inputs by giving them context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f872475",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Self Cross Attention.drawio.png\" width=2000> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Self Cross Attention.drawio.png\" width=2000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f16f75c0b5bae",
   "metadata": {},
   "source": [
    "## Encoder Block\n",
    "\n",
    "The encoder side of the transformer processes the input to the model. If your model is trained on translating sentences, the input will be the sentence in the source language. \n",
    "\n",
    "Each Block consists of \n",
    "\n",
    "1. **multi-head self-attention layer**\n",
    "2. **residual connection**\n",
    "3. **layer normalization**\n",
    "4. **feed forward network**\n",
    "5. **residual connection**\n",
    "6. **layer normalization**\n",
    "\n",
    "We use layer normalization instead of batch normalization to get similar advantages of improving training stability while decoupling it from the batch size. (Remember, in batch normalization we normalize over an entire batch, while in layer normalization we normalize across over all inputs of a single sample!)\n",
    "\n",
    "The feed forward network is applied to each token embedding separately with follwing architecture: \n",
    "\n",
    "$FFN(x) = RELU(xW_1 + b_1) \\cdot W_2 + b_2$,  where\n",
    "\n",
    "$shape(W_1) = (d_{model},\\, d_{ff})$ </br>\n",
    "$shape(W_2) = (d_{ff},\\, d_{model})$\n",
    "\n",
    "After this, another residual connection followed by a layer normalization is added.\n",
    "\n",
    "One intuitive way to think about this $MultiHead \\rightarrow FFN$ structure of the encoder block is that the attention mechanism is used to \"look around\" in the input sequence and the feed forward network is used to process the information and transform it into a more useful representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1971a5",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Encoder Block.drawio.png\" width=\"1300\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Encoder Block.drawio.png\" width=1300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb92f5506c45c2a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 3: Implement</h3>\n",
    "    <p>Implement the <code>__init__()</code> method and the <code>forward()</code> method of the <code>FeedForwardNeuralNetwork</code> class in <code>exercise_code/network/nn.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ab94c21859e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### Testing Started #######\n",
      "\n",
      "Test TestLinear1: \u001b[92mpassed!\u001b[0m\n",
      "Test TestLinear1Bias: \u001b[92mpassed!\u001b[0m\n",
      "Test TestLinear2: \u001b[92mpassed!\u001b[0m\n",
      "Test TestLinear2Bias: \u001b[92mpassed!\u001b[0m\n",
      "Test TestParameterCount: \u001b[92mpassed!\u001b[0m\n",
      "\n",
      "####### Testing Finished #######\n",
      "Test TestTask3: \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m5\u001b[0m/\u001b[92m5\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = test_task_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ce69f250dbc3e",
   "metadata": {},
   "source": [
    "<div class =\"alert alert-info\">\n",
    "    <h3>Task 4: Implement </h3>\n",
    "    <p>Implement the <code>__init__()</code> method and the <code>forward()</code> method of the <code>EncoderBlock</code> class in <code>exercise_code/network/encoder_block.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a6263d0344428b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### Testing Started #######\n",
      "\n",
      "Test EncoderBlockOutputShapeTest: \u001b[92mpassed!\u001b[0m\n",
      "Test EncoderBlockOutputNorm: \u001b[92mpassed!\u001b[0m\n",
      "Test EncoderBlockParameterCountTest: \u001b[92mpassed!\u001b[0m\n",
      "Test EncoderBlockValueTest: \u001b[92mpassed!\u001b[0m\n",
      "\n",
      "####### Testing Finished #######\n",
      "Test TestTask4: \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m4\u001b[0m/\u001b[92m4\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = test_task_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09703b84",
   "metadata": {},
   "source": [
    "## Encoder Stack\n",
    "\n",
    "The only part left on the encoder side is to stack multiple blocks together!\n",
    "\n",
    "<!-- <img src=\"images/Transformer-Encoder.drawio.png\" width=\"1000\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Encoder.drawio.png\" width=1000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ca607",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 5: Check Code</h3>\n",
    "    <p>Check the <code>__init__()</code> method and the <code>forward()</code> method of the <code>Encoder</code> class in <code>exercise_code/network/encoder.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74017db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### Testing Started #######\n",
      "\n",
      "Test EncoderOutputShapeTest: \u001b[92mpassed!\u001b[0m\n",
      "Test EncoderParameterCountTest: \u001b[92mpassed!\u001b[0m\n",
      "\n",
      "####### Testing Finished #######\n",
      "Test TestTask5: \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m2\u001b[0m/\u001b[92m2\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = test_task_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b46041",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "The job of the decoder is basically to produce an output given an input and the previous outputs (if available). Those previous outputs are also the input to the decoder! We have actually already seen most of the relevant parts of the decoder in the encoder, there is only one major addition: Causal attention!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b573662",
   "metadata": {},
   "source": [
    "## Causal Attention\n",
    "\n",
    "Previous language models based on RNN have one large draw back: during training, the model has to iteratively go through the entire sequence to predict the next word. Transformer models on the other hand can do this in parallel!\n",
    "\n",
    "So instead of something like this:\n",
    "\n",
    "|                    | Iteration 1                        | Iteration 2                        | Iteration 3                        |  \n",
    "|--------------------|------------------------------------|------------------------------------|------------------------------------|\n",
    "| **Encoder Input**  | [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"]| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"]| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"]|\n",
    "| **Decoder Input**  | [\"\\<start>\"]                       | [\"\\<start>\", \"Nicht\"]              | [\"\\<start>\", \"Nicht\", \"sehr\"]      | \n",
    "| **Decoder Output** | [\"Nicht\"]                          | [\"Nicht\", \"sehr\"]                  | [\"Nicht\", \"sehr\", \"effektiv\"]      | \n",
    "| **Compare to**     | [\"Hallo\"]                          | [\"Hallo\", \"wie\"]                   | [\"Hallo\", \"wie\", \"geht's\"]         | \n",
    "\n",
    "\n",
    "and so on we want to do this in one pass, where we give the model the correct sentence as the decoder input. It is shifted right, since the model should predict the first token, given the \\<start> token.\n",
    "\n",
    "|                    | Iteration 1                                        |\n",
    "|--------------------|----------------------------------------------------|\n",
    "| **Encoder Input**  | [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"]                |\n",
    "| **Decoder Input**  | [\"\\<start>\", \"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\"] | \n",
    "| **Decoder Output** | [\"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\", \"\\<end>\"]   | \n",
    "| **Compare to**     | [\"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\", \"\\<end>\"]   | \n",
    "\n",
    "\n",
    "Problem is, our model could theoretically learn to cheat, by just returning the same sequence it got as an input. In other words, we want to ensure, that when the model is predicting the token [\"geht's\"], it only depends on the previous token [\"\\<start>\", \"Hallo\", \"wie\"]. This can be done with masks!\n",
    "\n",
    "If we look back at our definition of attention we had:\n",
    "\n",
    "$\\tilde{x}_i = \\sum_{j=1}^N s_{ij} x_j$\n",
    "\n",
    "If we dont want future tokens to have an affect on the current token, we have to ensure, that all scores where $j > i$ are zero! (Normally it would be $\\geq$, but since the decoder input is shifted over by one token - the \\<start> token - its $>$)\n",
    "\n",
    "Example:\n",
    "\n",
    "$\\tilde{x}_0 = s_{00} x_0 + s_{01} x_1 +s_{02} x_2$ \\\n",
    "$\\tilde{x}_1 = s_{10} x_0 + s_{11} x_1 +s_{12} x_2$ \\\n",
    "$\\tilde{x}_2 = s_{20} x_0 + s_{21} x_1 +s_{22} x_2$  \n",
    "\n",
    "Now, we want  \n",
    "$\\tilde{x}_0$ to only depend on $x_0$  \n",
    "$\\tilde{x}_1$ to only depend on $x_0$ and $x_1$  \n",
    "$\\tilde{x}_2$ to only depend on $x_0$, $x_1$ and $x_2$  \n",
    "\n",
    "That leads to:  \n",
    "$\\tilde{x}_0 = s_{00} x_0 + 0 \\cdot x_1 + 0 \\cdot x_2$  \n",
    "$\\tilde{x}_1 = s_{10} x_0 + s_{11} x_1 + 0 \\cdot x_2$  \n",
    "$\\tilde{x}_2 = s_{20} x_0 + s_{21} x_1 +s_{22} x_2$  \n",
    "\n",
    "The scores form a lower triangle matrix. Let's test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8709b828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define scores as lower triangular matrix of ones\n",
    "scores = np.tril(np.ones((4, 4)), k=0)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb4411",
   "metadata": {},
   "source": [
    "Now that we know what we have to do, we have figure when to set these scores to zero! We could try right after we compute the dot products using a mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "878371f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [1. 1. 1. 0.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Define mask as lower triangular matrix of ones (same as above)\n",
    "mask = np.tril(np.ones((4, 4)), k=0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f03abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.5  0.   0.   0.  ]\n",
      " [2.75 2.75 0.   0.  ]\n",
      " [4.   3.25 5.75 0.  ]\n",
      " [3.25 3.25 5.   4.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Get dummy embeddings fo this example\n",
    "queries = util.get_dummy_embeddings()\n",
    "keys = util.get_dummy_embeddings()\n",
    "\n",
    "# Compute scores\n",
    "scores = queries @ keys.T\n",
    "\n",
    "# Multiply scores with mask to set all scores above the diagonal to zero\n",
    "scores = scores * mask\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680aa112",
   "metadata": {},
   "source": [
    "Looks good! But what happens when we run the softmax over it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa748c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.96774788 0.01075071 0.01075071 0.01075071]\n",
      " [0.46995667 0.46995667 0.03004333 0.03004333]\n",
      " [0.1380208  0.06519641 0.79425485 0.00252794]\n",
      " [0.10130067 0.10130067 0.58294513 0.21445353]]\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to scores\n",
    "scores = util.softmax(scores)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8006e35f",
   "metadata": {},
   "source": [
    "Suddenly our weights aren't zero anymore! This shouldn't be that big of a surprise, since $e^0 = 1$.\n",
    "Alright, let's try setting the scores to zero after the softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c269f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10588287 0.         0.         0.        ]\n",
      " [0.08691075 0.08691075 0.         0.        ]\n",
      " [0.09330698 0.04407509 0.53694458 0.        ]\n",
      " [0.10130067 0.10130067 0.58294513 0.21445353]]\n"
     ]
    }
   ],
   "source": [
    "# Compute scores\n",
    "scores = queries @ keys.T\n",
    "\n",
    "# Apply softmax to scores\n",
    "scores = util.softmax(scores)\n",
    "\n",
    "# Multiply scores with mask to set all scores above the diagonal to zero\n",
    "scores = scores * mask\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab35398",
   "metadata": {},
   "source": [
    "Alright, looks better! Only problem left is that the values don't add up to 1 anymore for every column! We somehow have to change the values before the softmax is applied!\n",
    "That's exactly what infinity mask do! Remember $e^{-inf} = 0$! (Technically it's the limit, but I think you know what we mean!) So these values wouldn't affect the sum, and their value will automatically be zero! So all we have to do is add -inf to the values we want to be zero later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21235cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0. -inf -inf -inf]\n",
      " [  0.   0. -inf -inf]\n",
      " [  0.   0.   0. -inf]\n",
      " [  0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# Replace all zeros with -inf (We want to keep the scores with a one!)\n",
    "inf_mask = np.where(mask, 0, -np.inf)\n",
    "print(inf_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc95f8",
   "metadata": {},
   "source": [
    "Let's add this to our scores before the softmax is applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2214367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         0.        ]\n",
      " [0.5        0.5        0.         0.        ]\n",
      " [0.13837059 0.06536164 0.79626777 0.        ]\n",
      " [0.10130067 0.10130067 0.58294513 0.21445353]]\n"
     ]
    }
   ],
   "source": [
    "# Compute scores\n",
    "scores = (queries @ keys.T) \n",
    "\n",
    "# Add the -inf mask to the scores\n",
    "scores += inf_mask\n",
    "\n",
    "# Apply softmax to scores\n",
    "scores = util.softmax(scores)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d55fa6",
   "metadata": {},
   "source": [
    "Perfect! Everything seems to work as planned!\n",
    "\n",
    "Note: This perticular kind of mask is known as a casual mask. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a42e82",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Masked Attention.drawio.png\" width=\"1000\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Masked Attention.drawio.png\" width=1000>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41b774",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 6: Implement</h3>\n",
    "    <p>Apply masking as explained above in the <code>forward()</code> method of the <code>ScaledDotAttention</code> class in <code>exercise_code/network/attention.py</code> and update the <code>forward()</code> method of the <code>MultiHeadAttention</code> class to pass the mask to the attention heads!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f47805b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### Testing Started #######\n",
      "\n",
      "Test AttentionPaddingTest: \u001b[92mpassed!\u001b[0m\n",
      "Test MultiHeadAttentionPaddingTest: \u001b[92mpassed!\u001b[0m\n",
      "\n",
      "####### Testing Finished #######\n",
      "Test TestTask6: \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m2\u001b[0m/\u001b[92m2\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = test_task_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc01d5e",
   "metadata": {},
   "source": [
    "## Decoder Block\n",
    "Just like the encoder, the decoder consists of several blocks. \n",
    "\n",
    "Each Block consists of \n",
    "\n",
    "1. **causal multi-head self-attention layer**\n",
    "2. **residual connection**\n",
    "3. **layer normalization**\n",
    "4. **multi-head cross-attention layer**\n",
    "5. **residual connection**\n",
    "6. **layer normalization**\n",
    "7. **feed forward network**\n",
    "8. **residual connection**\n",
    "9. **layer normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c603d49d",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-DecoderBlock.drawio.png\" width=\"1500\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Decoder Block.drawio.png\" width=1500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386125a4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 7: Implement</h3>\n",
    "    <p>Implement the <code>__init__()</code> method and the <code>forward()</code> method of the <code>DecoderBlock</code> class in <code>exercise_code/network/decoder_block.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "235605a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### Testing Started #######\n",
      "\n",
      "Test DecoderBlockOutputShapeTest: \u001b[92mpassed!\u001b[0m\n",
      "Test DecoderBlockOutputNorm: \u001b[92mpassed!\u001b[0m\n",
      "Test DecoderBlockParameterCountTest: \u001b[92mpassed!\u001b[0m\n",
      "Test DecoderBlockValueTest: \u001b[92mpassed!\u001b[0m\n",
      "\n",
      "####### Testing Finished #######\n",
      "Test TestTask7: \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m4\u001b[0m/\u001b[92m4\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = test_task_7()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e4c81",
   "metadata": {},
   "source": [
    "## Decoder Stack\n",
    "\n",
    "Just like in the encoder, the decoder consist of several decoder blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef132331",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Decoder.drawio.png\" width=\"1000\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Decoder.drawio.png\" width=1000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37d1b2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 8: Check Code</h3>\n",
    "    <p>Check the <code>__init__()</code> method and the <code>forward()</code> method of the <code>Decoder</code> class in <code>exercise_code/network/decoder.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a9370b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### Testing Started #######\n",
      "\n",
      "Test DecoderOutputShapeTest: \u001b[92mpassed!\u001b[0m\n",
      "Test DecoderParameterCountTest: \u001b[92mpassed!\u001b[0m\n",
      "\n",
      "####### Testing Finished #######\n",
      "Test TestTask8: \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m2\u001b[0m/\u001b[92m2\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = test_task_8()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a911f69",
   "metadata": {},
   "source": [
    "## Final Output\n",
    "\n",
    "The last thing we have to do is project the individual embeddings into distributions over our vocabulary. This can be done with a simple linear layer! The outputs at this stage will not be distributions yet, since the values do not add up to one! We accomplish this in the loss layer, using a softmax function!\n",
    "\n",
    "To minimize the weights needed in this network, we will actually use a technique called weight tying! If you remember the Embedding Layer, this was basically a weight matrix with shape (vocab_size, d_model). For our final output layer, we want to project from the embedding space to the vocabulary space, so that gives us (d_model, vocab_size)! The shapes are just transposed! And the approach is to actually reuse these weights from our embeddings and transpose them for our final layer! Here is the corresponding paper: https://arxiv.org/abs/1608.05859v3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd491ec",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Transformer-Full.drawio.png\" width=\"2500\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Transformer-Full.drawio.png\" width=2500>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5778c96",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 9: Implement</h3>\n",
    "    <p>Implement the <code>__init__()</code> and the <code>forward()</code> method of the <code>Transformer</code> class in <code>exercise_code/network/transformer.py</code>. We have already implemented weight tying for you!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c9e6a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### Testing Started #######\n",
      "\n",
      "Test TransformerOutputShapeTest: \u001b[92mpassed!\u001b[0m\n",
      "Test TransformerParameterCountTest: \u001b[92mpassed!\u001b[0m\n",
      "Test TransformerParameterCountWeightTyingTest: \u001b[92mpassed!\u001b[0m\n",
      "Test TransformerValueTest: \u001b[92mpassed!\u001b[0m\n",
      "\n",
      "####### Testing Finished #######\n",
      "Test TestTask9: \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m4\u001b[0m/\u001b[92m4\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = test_task_9()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569366d6",
   "metadata": {},
   "source": [
    "# Paddings\n",
    "\n",
    "A keen eye might have noticed a problem with our model - in the collate function we added paddings to ensure all sequences have the same length! Our model on the other hand shouldn't change its output just because we are adding \"empty\" tokens at the end of our sequence! The good news is, we have actually already implemented most of what we need to actually enable this! But first, let's have a look at what we are dealing with and load in a batch from our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3116f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data import CustomIterableDataset\n",
    "from exercise_code.data import CustomCollator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the path to the dataset\n",
    "file = os.path.join(dataset_path, 'dummyDatasets', 'ds_dummy')\n",
    "\n",
    "# Define the collator and dataset\n",
    "collator = CustomCollator()\n",
    "dataset = CustomIterableDataset(file)\n",
    "\n",
    "# Define the data loader\n",
    "loader = DataLoader(dataset, batch_size=3, collate_fn=collator)\n",
    "\n",
    "# Create an Embedding layer with 512 dimensions\n",
    "embedding = Embedding(vocab_size=len(collator.tokenizer), d_model=512, max_length=2048)\n",
    "\n",
    "# Get the first batch from the data loader\n",
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c4025",
   "metadata": {},
   "source": [
    "Note: If this didn't work, there is probably a problem in your Dataset! Go Back to Notebook 2 and make sure you pass the test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deb47e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True]]\n",
      "\n",
      " [[ True  True  True  True  True  True  True  True  True  True False\n",
      "   False]]\n",
      "\n",
      " [[ True  True  True  True  True  True  True  True  True False False\n",
      "   False]]]\n"
     ]
    }
   ],
   "source": [
    "padding_masks = batch['encoder_mask']\n",
    "\n",
    "print(padding_masks.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089927b",
   "metadata": {},
   "source": [
    "In this first item of the batch, the sequence has no padding at all - so nothing to do here!\n",
    "For all the others we have come up with something... Let's go back to the formula:\n",
    "\n",
    "$\\tilde{x}_i = \\sum_{j=1}^N s_{ij} x_j$\n",
    "\n",
    "All we really want is that if $x_j$ is a padding token, it doesn't contribute to the updated embedding! In other words - its score has to be zero! We can solve this exactly the same way as we did with the attention masking! Let's do this for a single item, in this case the second item in the batch!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f7edee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 512])\n"
     ]
    }
   ],
   "source": [
    "# Load the embeddings from the second item in the batch\n",
    "inputs = embedding(batch['encoder_inputs'][1])\n",
    "\n",
    "# We are using the same embeddings for the queries, keys and values - self-attention!\n",
    "queries = inputs\n",
    "keys = inputs\n",
    "values = inputs\n",
    "\n",
    "print(queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f737bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False, False])\n"
     ]
    }
   ],
   "source": [
    "# Load the padding mask of the second item\n",
    "padding_mask = padding_masks[1].squeeze(0)\n",
    "\n",
    "print(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cbcb3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a mask of length 12 by 12 and set the values to 0 where we have to mask\n",
    "mask = torch.ones((len(padding_mask), len(padding_mask)))\n",
    "\n",
    "for i, row in enumerate(mask):\n",
    "    for j, item in enumerate(row):\n",
    "        if not padding_mask[j]:\n",
    "            mask[i, j] = 0\n",
    "    \n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbe5e1",
   "metadata": {},
   "source": [
    "What we did is set the mask to zero, for every column that refers to a padding token! What we get is this matrix, where the left side are ones, and the rest is zero. We can achieve the same result by just copying the vector along the first dimension - duh! (We will let pytorch handle the copying automatically using broadcasting, a keen eye might have noticed that we squeezed the dimension two cells up ;))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d325b6",
   "metadata": {},
   "source": [
    "From here we can treat it the same way we did with our causal attention block, by adding -inf to all values we want to mask out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb33053f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Compute scores\n",
    "scores = (queries @ keys.T)/np.sqrt(512)\n",
    "\n",
    "# Instead of adding -inf, we set the scores to -inf where the mask is 0 -> Same thing ;)\n",
    "scores.masked_fill_(~mask.bool(), -torch.inf)\n",
    "\n",
    "# Apply softmax to scores\n",
    "scores = softmax(scores, dim=-1)\n",
    "\n",
    "# Print scores\n",
    "scores = scores.detach().numpy()\n",
    "util.plot_attention_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec363f",
   "metadata": {},
   "source": [
    "Perfect, the scores are concentrated to the left side!\n",
    "For the causal mask (the lower triangle mask) we have to combine it with the decoder mask. We can do this by simply multiplying the two together!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e75ba46e26ab345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from exercise_code.util.transformer_util import create_causal_mask\n",
    "\n",
    "# Get the masks from the first batch\n",
    "encoder_mask = batch['encoder_mask']\n",
    "decoder_mask = batch['decoder_mask']\n",
    "\n",
    "# Create the causal mask (lower triangle mask) for the encoder\n",
    "causal_mask = create_causal_mask(decoder_mask.shape[-1])\n",
    "\n",
    "# Combine the decoder mask and the causal mask\n",
    "causal_mask = causal_mask * decoder_mask\n",
    "\n",
    "# Plot all masks\n",
    "util.plot_boolean_masks(causal_mask, encoder_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664a5816e3591f1",
   "metadata": {},
   "source": [
    "Note: For those who are wondering why the shape dont match up: The decoder mask and encoder mask dont have to be the same length for each sentence ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99655b8d79e74dd8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 10: Implement</h3>\n",
    "    <p>Add the padding masks at the appropriate spots in the code! Please go over the <code>forward()</code> passes in the <code>EncoderBlock</code>, <code>DecoderBlock</code> as well as the <code>Transformer</code> class in their respective files!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b05c25b7454b945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### Testing Started #######\n",
      "\n",
      "Test TransformerPaddingTest: \u001b[92mpassed!\u001b[0m\n",
      "\n",
      "####### Testing Finished #######\n",
      "Test TestTask10: \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m1\u001b[0m/\u001b[92m1\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = test_task_10()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d54a9d",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    " We employ two types of regularization during training:\n",
    " \n",
    "- Residual Dropout: \n",
    "We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of $P_{drop}$ = 0.1.\n",
    " \n",
    "- Label Smoothing: \n",
    "During training, we employed label smoothing of value $\\epsilon_{ls}$ = 0.1. This hurts perplexity, as the model learns to be more unsure, but improves accuracy. This means, that instead of using labels with one-hot encoding:\n",
    "\n",
    "$ y_{1hot} = \\begin{bmatrix}0 & 0 & 0 & \\cdots & 1 & \\cdots & 0 & 0 \\end{bmatrix} $\n",
    "\n",
    "Instead of zeros we use a small value $s = \\frac{\\epsilon_{ls}}{n_{cls} - 1}$, where $n_{cls}$ is the number of classes (=vocab_size). Since this has to be a proper distribution this has to add up 1. This results in a probability of being the correct word $p = 1 - \\epsilon_{ls}$\n",
    "\n",
    "For $n_{cls} = 11$ and $\\epsilon_ls = 0.1$, this would result in:\n",
    "\n",
    "$ y_{smooth} = \\begin{bmatrix}0.01 & 0.01 & 0.01 & 0.01 & 0.9 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f4d18d8abf0ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 11: Implement</h3>\n",
    "    <p>Initialize dropout in the classes <code>Embedding</code>, <code>ScaledDotAttention</code>, <code>MultiHeadAttention</code> and <code>FeedForwardNeuralNetwork</code> in their respective files. Don't forget to add it in the <code>forward()</code> pass! \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5428624bc79f038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### Testing Started #######\n",
      "\n",
      "Test EmbeddingDropoutTest: \u001b[92mpassed!\u001b[0m\n",
      "Test AttentionDropoutTest: \u001b[92mpassed!\u001b[0m\n",
      "Test MultiHeadDropoutTest: \u001b[92mpassed!\u001b[0m\n",
      "Test FeedForwardNeuralNetworkDropoutTest: \u001b[92mpassed!\u001b[0m\n",
      "\n",
      "####### Testing Finished #######\n",
      "Test TestTask11: \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m4\u001b[0m/\u001b[92m4\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# If you get an Error about SCORE_SAVER - please just restart your kernel!\n",
    "_ = test_task_11()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bf61fd0713dea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 12: Check Code</h3>\n",
    "    <p>Have a look at <code>SmoothCrossEntropy</code> in <code>exercise_code/network/loss.py</code>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03cdca13956f142",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "The paper used the Adam optimizer with $\\beta_1 = 0.9$, $\\beta_2 = 0.98$ and $\\epsilon = 10^{-9}$. </br>\n",
    "They varied the learning rate over the course of training, according to the formula: \n",
    "\n",
    "$lrate = d_{model}^{-0.5} \\cdot min(step\\_num^{0.5}, step\\_num \\cdot warmup\\_steps^{1.5})$\n",
    "\n",
    "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae400325ec6e7e1e",
   "metadata": {},
   "source": [
    "The setup could look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c184f04774015836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from exercise_code.data.tokenizer import load_pretrained_fast\n",
    "\n",
    "tokenizer = load_pretrained_fast()\n",
    "model = Transformer(vocab_size=len(tokenizer), eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "d_model = model.d_model\n",
    "lr_start =d_model**-0.5\n",
    "eps=1e-9\n",
    "betas=(0.9, 0.98)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr_start, eps=eps, betas=betas)\n",
    "\n",
    "warm_up = 4000\n",
    "lr_lambda=lambda step: min((step+1)**-0.5, (step+1)*warm_up**-1.5)\n",
    "scheduler_example = LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf3e53da9001a1",
   "metadata": {},
   "source": [
    "Let's have a look at this scheduler function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "810428a45c4d7b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = np.arange(0, 100000)\n",
    "lr = lr_start * np.vectorize(lr_lambda)(steps)\n",
    "\n",
    "plt.plot(steps, lr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392f53e1d796c23",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Like already mentioned - Training is really where Transformers show there advantages compared to other sequential models! We can use a technique called **Teacher Forcing**, which means we feed the model the correct sentence, and the model only has to predict the next word! This is super easy to do in Transformers due to the causal masks we discussed earlier! In fact, all we have to do is pass the entire correct sentence into the decoder, and let it run exactly one time. The masking will make sure, each predicted word only depends on the previous words - no need to loop!\n",
    "\n",
    "For this exercise, we will not train a large model on a huge dataset, this would simply take too long and also isn't the focus of this exercise! However, you will have to implement the parts of the Trainer class, particular the forward pass! Note that have implemented a couple of extra functionalities, such as training from a checkpoint and a gradient accumulation. Gradient accumulation is used, to decouple the batch size from the optimizing step. Instead of performing an update after every batch, we can update our network after n batches. That way, we can choose a smaller batch size to save RAM, and still have the same effects as we would have with a larger batch! Be aware, that this doesn't work as nicely with batch normalization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c47b90e4ad0b77",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 13: Implement</h3>\n",
    "    <p>Implement the <code>_forward()</code> method in the Trainer class in <code>exercise_code/trainer.py</code>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64baecd09e2e9cf",
   "metadata": {},
   "source": [
    "Now we will overfit a small model on a dummy Dataset with small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "635e404efc259d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.Trainer import Trainer\n",
    "from exercise_code.network import SmoothCrossEntropyLoss\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from exercise_code.data.tokenizer import load_pretrained_fast\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = load_pretrained_fast()\n",
    "\n",
    "hparams = {\n",
    "    'd_model': 128,\n",
    "    'd_k': 32,\n",
    "    'd_v': 32,\n",
    "    'n_heads': 4,\n",
    "    'd_ff': 512,\n",
    "    'n': 4,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Implement the model                                          \n",
    "model = Transformer(vocab_size=len(tokenizer), \n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    hparams=hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd36935f75405a",
   "metadata": {},
   "source": [
    "Alright, let's check the model size! For this task, the model should have less than 5 million parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "debc89d5eb4b5b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model has 2893824 parameters.\n",
      "Test TestModelParameters: \u001b[92mpassed!\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = test_model_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8228df541ae40e7a",
   "metadata": {},
   "source": [
    "Alright, now we can define the dataset and the dataloader. The dataset only contains 1000 lines. We will also initialize the Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b878608289c10f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "#   Define the optimizer and optionally scheduler - not really needed  #                                                                     #\n",
    "########################################################################\n",
    "\n",
    "d_model = model.d_model\n",
    "lr_start = d_model**-0.5\n",
    "eps=1e-9\n",
    "betas=(0.9, 0.98)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr_start, eps=eps, betas=betas)\n",
    "\n",
    "warm_up = 500\n",
    "lr_lambda=lambda step: min((step+1)**-0.5, (step+1)*warm_up**-1.5)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "loss_func = SmoothCrossEntropyLoss(smoothing=0.1)\n",
    "file_path = os.path.join(dataset_path, 'dummyDatasets', 'ds_dummy')\n",
    "collator = CustomCollator(tokenizer=tokenizer)\n",
    "dataset = CustomIterableDataset(file_path)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collator)\n",
    "\n",
    "# For Apple M1/M2/M3 users: Try out the MPS framework, it will significantly speed up your training!\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif MPS_AVAILABLE:\n",
    "    if torch.backends.mps.is_available(): \n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  loss_func=loss_func,\n",
    "                  train_loader=dataloader,\n",
    "                  val_loader=None,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=scheduler,\n",
    "                  epochs=epochs,\n",
    "                  device=device,\n",
    "                  optimizer_interval=0, # If you want to enable gradient accumulation, you can set this parameter! \n",
    "                  checkpoint_interval=0) # If you want to store your progress. You can resume training using train_from_checkpoint(#folder_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8dd099294456d",
   "metadata": {},
   "source": [
    "Now let's finally train the model! To pass this task, you will need at least 50% accuracy on the dataset! Note: Don't be surprised if this will take a few epochs (>50 probably) and the accuracy will get better very slowly, especially in the beginning! Just let it run, we are overfitting on purpose so it should usually always work at some point!\n",
    "\n",
    "To explain the metrics: The first number is always the loss / accuracy over the current batch and the second number is always computed over the entire epoch.\n",
    "You can also resume training by just executing the cell again. If you reached the end of your epochs and run it again, it will also continue, starting where you left of for as many epochs as you configured. If you stop this cell, your models parameters will not be altered - in other words if you see you have reached the accuracy, just stop the cell ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "974182aeee4cda2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/300: 100%|| 7/7 [00:01<00:00,  6.81 batches/s, loss=65.012/74.384, train accuracy=0.000/0.345, learning_rate=5.534e-05]\n",
      "Training Epoch 2/300: 100%|| 7/7 [00:00<00:00, 12.36 batches/s, loss=37.372/48.829, train accuracy=0.000/0.207, learning_rate=1.107e-04]\n",
      "Training Epoch 3/300: 100%|| 7/7 [00:00<00:00, 10.62 batches/s, loss=29.491/34.814, train accuracy=0.000/0.000, learning_rate=1.660e-04]\n",
      "Training Epoch 4/300: 100%|| 7/7 [00:00<00:00, 11.30 batches/s, loss=24.225/29.845, train accuracy=0.000/0.000, learning_rate=2.214e-04]\n",
      "Training Epoch 5/300: 100%|| 7/7 [00:00<00:00, 10.72 batches/s, loss=21.241/27.348, train accuracy=0.000/0.000, learning_rate=2.767e-04]\n",
      "Training Epoch 6/300: 100%|| 7/7 [00:00<00:00, 10.99 batches/s, loss=19.396/25.526, train accuracy=0.000/0.000, learning_rate=3.320e-04]\n",
      "Training Epoch 7/300: 100%|| 7/7 [00:00<00:00, 10.78 batches/s, loss=17.649/24.018, train accuracy=0.000/0.828, learning_rate=3.874e-04]\n",
      "Training Epoch 8/300: 100%|| 7/7 [00:00<00:00, 10.98 batches/s, loss=16.546/22.730, train accuracy=7.143/0.483, learning_rate=4.427e-04]\n",
      "Training Epoch 9/300: 100%|| 7/7 [00:00<00:00, 10.70 batches/s, loss=16.049/21.782, train accuracy=7.143/1.311, learning_rate=4.981e-04]\n",
      "Training Epoch 10/300: 100%|| 7/7 [00:00<00:00, 12.54 batches/s, loss=15.128/20.857, train accuracy=0.000/2.139, learning_rate=5.534e-04]\n",
      "Training Epoch 11/300: 100%|| 7/7 [00:00<00:00, 10.43 batches/s, loss=14.445/20.049, train accuracy=0.000/2.277, learning_rate=6.087e-04]\n",
      "Training Epoch 12/300: 100%|| 7/7 [00:00<00:00, 11.40 batches/s, loss=13.868/19.247, train accuracy=0.000/2.622, learning_rate=6.641e-04]\n",
      "Training Epoch 13/300: 100%|| 7/7 [00:00<00:00, 12.57 batches/s, loss=13.136/18.467, train accuracy=7.143/2.208, learning_rate=7.194e-04]\n",
      "Training Epoch 14/300: 100%|| 7/7 [00:00<00:00, 12.03 batches/s, loss=12.433/17.708, train accuracy=14.286/2.070, learning_rate=7.748e-04]\n",
      "Training Epoch 15/300: 100%|| 7/7 [00:00<00:00, 11.84 batches/s, loss=11.697/16.951, train accuracy=7.143/2.208, learning_rate=8.301e-04]\n",
      "Training Epoch 16/300: 100%|| 7/7 [00:00<00:00, 13.00 batches/s, loss=10.909/16.181, train accuracy=7.143/2.761, learning_rate=8.854e-04]\n",
      "Training Epoch 17/300: 100%|| 7/7 [00:00<00:00, 12.94 batches/s, loss=10.092/15.408, train accuracy=7.143/3.313, learning_rate=9.408e-04]\n",
      "Training Epoch 18/300: 100%|| 7/7 [00:00<00:00, 13.55 batches/s, loss=8.965/14.589, train accuracy=14.286/4.003, learning_rate=9.961e-04]\n",
      "Training Epoch 19/300: 100%|| 7/7 [00:00<00:00, 13.71 batches/s, loss=7.733/13.771, train accuracy=14.286/4.210, learning_rate=1.051e-03]\n",
      "Training Epoch 20/300: 100%|| 7/7 [00:00<00:00, 14.02 batches/s, loss=6.412/13.005, train accuracy=21.429/5.107, learning_rate=1.107e-03]\n",
      "Training Epoch 21/300: 100%|| 7/7 [00:00<00:00, 13.66 batches/s, loss=5.482/12.703, train accuracy=28.571/3.037, learning_rate=1.162e-03]\n",
      "Training Epoch 22/300: 100%|| 7/7 [00:00<00:00, 14.41 batches/s, loss=5.404/12.445, train accuracy=21.429/4.141, learning_rate=1.217e-03]\n",
      "Training Epoch 23/300: 100%|| 7/7 [00:00<00:00, 13.42 batches/s, loss=4.350/11.557, train accuracy=42.857/4.969, learning_rate=1.273e-03]\n",
      "Training Epoch 24/300: 100%|| 7/7 [00:00<00:00, 14.03 batches/s, loss=4.575/10.913, train accuracy=42.857/6.625, learning_rate=1.328e-03] \n",
      "Training Epoch 25/300: 100%|| 7/7 [00:00<00:00, 13.70 batches/s, loss=2.279/9.999, train accuracy=85.714/7.591, learning_rate=1.383e-03] \n",
      "Training Epoch 26/300: 100%|| 7/7 [00:00<00:00, 13.68 batches/s, loss=2.642/9.394, train accuracy=71.429/8.144, learning_rate=1.439e-03] \n",
      "Training Epoch 27/300: 100%|| 7/7 [00:00<00:00, 14.23 batches/s, loss=2.245/8.852, train accuracy=92.857/9.248, learning_rate=1.494e-03] \n",
      "Training Epoch 28/300: 100%|| 7/7 [00:00<00:00, 14.07 batches/s, loss=1.942/8.314, train accuracy=92.857/9.800, learning_rate=1.550e-03]\n",
      "Training Epoch 29/300: 100%|| 7/7 [00:00<00:00, 14.00 batches/s, loss=1.855/7.918, train accuracy=100.000/10.697, learning_rate=1.605e-03]\n",
      "Training Epoch 30/300: 100%|| 7/7 [00:00<00:00, 13.65 batches/s, loss=1.766/7.392, train accuracy=100.000/13.251, learning_rate=1.660e-03]\n",
      "Training Epoch 31/300: 100%|| 7/7 [00:00<00:00, 13.98 batches/s, loss=1.734/6.871, train accuracy=100.000/14.079, learning_rate=1.716e-03]\n",
      "Training Epoch 32/300: 100%|| 7/7 [00:00<00:00, 14.98 batches/s, loss=1.867/6.571, train accuracy=100.000/15.045, learning_rate=1.771e-03]\n",
      "Training Epoch 33/300: 100%|| 7/7 [00:00<00:00, 14.34 batches/s, loss=1.915/6.393, train accuracy=100.000/15.045, learning_rate=1.826e-03]\n",
      "Training Epoch 34/300: 100%|| 7/7 [00:00<00:00, 13.62 batches/s, loss=2.712/6.350, train accuracy=85.714/14.424, learning_rate=1.882e-03]\n",
      "Training Epoch 35/300: 100%|| 7/7 [00:00<00:00, 15.23 batches/s, loss=2.113/5.919, train accuracy=92.857/17.943, learning_rate=1.937e-03]\n",
      "Training Epoch 36/300: 100%|| 7/7 [00:00<00:00, 14.43 batches/s, loss=3.079/5.647, train accuracy=78.571/18.427, learning_rate=1.992e-03]\n",
      "Training Epoch 37/300: 100%|| 7/7 [00:00<00:00, 13.33 batches/s, loss=3.508/5.640, train accuracy=78.571/19.393, learning_rate=2.048e-03]\n",
      "Training Epoch 38/300: 100%|| 7/7 [00:00<00:00, 13.22 batches/s, loss=1.979/4.952, train accuracy=85.714/24.293, learning_rate=2.103e-03]\n",
      "Training Epoch 39/300: 100%|| 7/7 [00:00<00:00, 11.35 batches/s, loss=2.825/4.759, train accuracy=78.571/25.742, learning_rate=2.158e-03]\n",
      "Training Epoch 40/300: 100%|| 7/7 [00:00<00:00, 11.57 batches/s, loss=1.874/4.484, train accuracy=85.714/27.260, learning_rate=2.214e-03]\n",
      "Training Epoch 41/300: 100%|| 7/7 [00:00<00:00, 11.14 batches/s, loss=1.933/4.229, train accuracy=100.000/29.676, learning_rate=2.269e-03]\n",
      "Training Epoch 42/300: 100%|| 7/7 [00:00<00:00, 11.28 batches/s, loss=2.034/4.084, train accuracy=100.000/30.711, learning_rate=2.324e-03]\n",
      "Training Epoch 43/300: 100%|| 7/7 [00:00<00:00, 11.60 batches/s, loss=2.080/3.763, train accuracy=92.857/36.439, learning_rate=2.380e-03]\n",
      "Training Epoch 44/300: 100%|| 7/7 [00:00<00:00, 11.34 batches/s, loss=1.666/3.598, train accuracy=100.000/40.097, learning_rate=2.435e-03]\n",
      "Training Epoch 45/300: 100%|| 7/7 [00:00<00:00, 10.92 batches/s, loss=1.639/3.330, train accuracy=100.000/43.133, learning_rate=2.490e-03]\n",
      "Training Epoch 46/300: 100%|| 7/7 [00:00<00:00, 10.41 batches/s, loss=1.667/3.109, train accuracy=100.000/47.412, learning_rate=2.546e-03]\n",
      "Training Epoch 47/300: 100%|| 7/7 [00:00<00:00, 11.53 batches/s, loss=1.878/3.027, train accuracy=100.000/52.312, learning_rate=2.601e-03]\n",
      "Training Epoch 48/300: 100%|| 7/7 [00:00<00:00, 11.26 batches/s, loss=1.803/2.790, train accuracy=92.857/57.419, learning_rate=2.656e-03]\n",
      "Training Epoch 49/300: 100%|| 7/7 [00:00<00:00, 11.28 batches/s, loss=1.858/2.635, train accuracy=100.000/62.112, learning_rate=2.712e-03]\n",
      "Training Epoch 50/300: 100%|| 7/7 [00:00<00:00, 11.63 batches/s, loss=1.833/2.494, train accuracy=100.000/65.631, learning_rate=2.767e-03]\n",
      "Training Epoch 51/300: 100%|| 7/7 [00:00<00:00, 11.41 batches/s, loss=1.803/2.396, train accuracy=100.000/70.945, learning_rate=2.822e-03]\n",
      "Training Epoch 52/300: 100%|| 7/7 [00:00<00:00, 11.15 batches/s, loss=1.695/2.342, train accuracy=100.000/70.876, learning_rate=2.878e-03]\n",
      "Training Epoch 53/300: 100%|| 7/7 [00:00<00:00, 10.95 batches/s, loss=1.712/2.212, train accuracy=92.857/77.433, learning_rate=2.933e-03]\n",
      "Training Epoch 54/300: 100%|| 7/7 [00:00<00:00, 11.37 batches/s, loss=2.067/2.221, train accuracy=92.857/78.813, learning_rate=2.988e-03]\n",
      "Training Epoch 55/300: 100%|| 7/7 [00:00<00:00, 11.15 batches/s, loss=2.261/2.253, train accuracy=92.857/78.813, learning_rate=3.044e-03]\n",
      "Training Epoch 56/300: 100%|| 7/7 [00:00<00:00, 11.73 batches/s, loss=2.242/2.268, train accuracy=85.714/77.847, learning_rate=3.099e-03]\n",
      "Training Epoch 57/300: 100%|| 7/7 [00:00<00:00, 11.69 batches/s, loss=1.789/2.220, train accuracy=100.000/79.089, learning_rate=3.154e-03]\n",
      "Training Epoch 58/300: 100%|| 7/7 [00:00<00:00, 11.84 batches/s, loss=2.746/2.313, train accuracy=78.571/80.538, learning_rate=3.210e-03]\n",
      "Training Epoch 59/300: 100%|| 7/7 [00:00<00:00, 11.41 batches/s, loss=2.432/2.330, train accuracy=85.714/77.778, learning_rate=3.265e-03]\n",
      "Training Epoch 60/300: 100%|| 7/7 [00:00<00:00, 11.02 batches/s, loss=2.084/2.279, train accuracy=92.857/77.364, learning_rate=3.320e-03]\n",
      "Training Epoch 61/300: 100%|| 7/7 [00:00<00:00, 11.14 batches/s, loss=2.375/2.201, train accuracy=85.714/81.919, learning_rate=3.376e-03]\n",
      "Training Epoch 62/300: 100%|| 7/7 [00:00<00:00, 11.24 batches/s, loss=2.069/2.156, train accuracy=85.714/80.331, learning_rate=3.431e-03]\n",
      "Training Epoch 63/300: 100%|| 7/7 [00:00<00:00, 11.44 batches/s, loss=3.117/2.321, train accuracy=78.571/81.090, learning_rate=3.486e-03]\n",
      "Training Epoch 64/300: 100%|| 7/7 [00:00<00:00, 11.33 batches/s, loss=2.110/2.222, train accuracy=78.571/81.090, learning_rate=3.542e-03]\n",
      "Training Epoch 65/300: 100%|| 7/7 [00:00<00:00, 11.35 batches/s, loss=3.169/2.385, train accuracy=57.143/79.848, learning_rate=3.597e-03]\n",
      "Training Epoch 66/300: 100%|| 7/7 [00:00<00:00, 11.33 batches/s, loss=3.182/2.619, train accuracy=71.429/74.120, learning_rate=3.652e-03]\n",
      "Training Epoch 67/300: 100%|| 7/7 [00:00<00:00, 11.31 batches/s, loss=2.338/2.731, train accuracy=78.571/68.806, learning_rate=3.708e-03]\n",
      "Training Epoch 68/300: 100%|| 7/7 [00:00<00:00, 10.79 batches/s, loss=1.927/2.696, train accuracy=100.000/64.251, learning_rate=3.763e-03]\n",
      "Training Epoch 69/300: 100%|| 7/7 [00:00<00:00, 10.41 batches/s, loss=2.426/2.362, train accuracy=71.429/77.433, learning_rate=3.818e-03]\n",
      "Training Epoch 70/300: 100%|| 7/7 [00:00<00:00, 11.86 batches/s, loss=1.920/2.095, train accuracy=92.857/85.093, learning_rate=3.874e-03]\n",
      "Training Epoch 71/300: 100%|| 7/7 [00:00<00:00, 10.91 batches/s, loss=1.991/2.075, train accuracy=100.000/85.369, learning_rate=3.929e-03]\n",
      "Training Epoch 72/300: 100%|| 7/7 [00:00<00:00, 12.45 batches/s, loss=1.903/2.060, train accuracy=100.000/84.955, learning_rate=3.937e-03]\n",
      "Training Epoch 73/300: 100%|| 7/7 [00:00<00:00, 11.68 batches/s, loss=2.212/1.973, train accuracy=78.571/90.269, learning_rate=3.910e-03]\n",
      "Training Epoch 74/300: 100%|| 7/7 [00:00<00:00, 10.90 batches/s, loss=2.244/1.993, train accuracy=92.857/89.993, learning_rate=3.884e-03]\n",
      "Training Epoch 75/300: 100%|| 7/7 [00:00<00:00, 11.14 batches/s, loss=3.629/2.172, train accuracy=57.143/88.475, learning_rate=3.858e-03]\n",
      "Training Epoch 76/300: 100%|| 7/7 [00:00<00:00, 10.79 batches/s, loss=2.710/2.051, train accuracy=71.429/89.234, learning_rate=3.832e-03]\n",
      "Training Epoch 77/300: 100%|| 7/7 [00:00<00:00, 11.74 batches/s, loss=3.859/2.252, train accuracy=64.286/87.716, learning_rate=3.807e-03]\n",
      "Training Epoch 78/300: 100%|| 7/7 [00:00<00:00, 11.27 batches/s, loss=2.706/2.118, train accuracy=92.857/88.061, learning_rate=3.783e-03]\n",
      "Training Epoch 79/300: 100%|| 7/7 [00:00<00:00, 11.38 batches/s, loss=3.240/2.093, train accuracy=71.429/90.545, learning_rate=3.759e-03]\n",
      "Training Epoch 80/300: 100%|| 7/7 [00:00<00:00, 10.73 batches/s, loss=1.729/1.852, train accuracy=92.857/91.511, learning_rate=3.735e-03]\n",
      "Training Epoch 81/300: 100%|| 7/7 [00:00<00:00, 12.87 batches/s, loss=1.663/1.776, train accuracy=100.000/92.478, learning_rate=3.712e-03]\n",
      "Training Epoch 82/300: 100%|| 7/7 [00:00<00:00, 11.30 batches/s, loss=1.552/1.795, train accuracy=100.000/93.237, learning_rate=3.689e-03]\n",
      "Training Epoch 83/300: 100%|| 7/7 [00:00<00:00, 11.01 batches/s, loss=1.577/1.731, train accuracy=100.000/94.479, learning_rate=3.667e-03]\n",
      "Training Epoch 84/300: 100%|| 7/7 [00:00<00:00, 11.66 batches/s, loss=1.614/1.686, train accuracy=100.000/96.411, learning_rate=3.645e-03]\n",
      "Training Epoch 85/300: 100%|| 7/7 [00:00<00:00, 10.92 batches/s, loss=1.960/1.719, train accuracy=92.857/97.239, learning_rate=3.624e-03]\n",
      "Training Epoch 86/300: 100%|| 7/7 [00:00<00:00, 11.11 batches/s, loss=1.898/1.702, train accuracy=85.714/96.894, learning_rate=3.602e-03]\n",
      "Training Epoch 87/300: 100%|| 7/7 [00:00<00:00, 11.15 batches/s, loss=2.880/1.898, train accuracy=78.571/95.445, learning_rate=3.582e-03]\n",
      "Training Epoch 88/300: 100%|| 7/7 [00:00<00:00, 10.75 batches/s, loss=2.266/1.865, train accuracy=85.714/93.099, learning_rate=3.561e-03]\n",
      "Training Epoch 89/300: 100%|| 7/7 [00:00<00:00, 11.37 batches/s, loss=2.954/1.970, train accuracy=92.857/93.306, learning_rate=3.541e-03]\n",
      "Training Epoch 90/300: 100%|| 7/7 [00:00<00:00, 11.82 batches/s, loss=2.465/1.886, train accuracy=78.571/93.582, learning_rate=3.521e-03]\n",
      "Training Epoch 91/300: 100%|| 7/7 [00:00<00:00, 12.59 batches/s, loss=2.315/1.899, train accuracy=85.714/91.718, learning_rate=3.502e-03]\n",
      "Training Epoch 92/300: 100%|| 7/7 [00:00<00:00, 11.71 batches/s, loss=3.461/2.101, train accuracy=78.571/91.373, learning_rate=3.483e-03]\n",
      "Training Epoch 93/300: 100%|| 7/7 [00:00<00:00, 11.18 batches/s, loss=1.834/1.891, train accuracy=85.714/90.890, learning_rate=3.464e-03]\n",
      "Training Epoch 94/300: 100%|| 7/7 [00:00<00:00, 10.86 batches/s, loss=1.975/1.878, train accuracy=85.714/92.409, learning_rate=3.446e-03]\n",
      "Training Epoch 95/300: 100%|| 7/7 [00:00<00:00, 11.44 batches/s, loss=2.440/1.888, train accuracy=85.714/93.099, learning_rate=3.428e-03]\n",
      "Training Epoch 96/300: 100%|| 7/7 [00:00<00:00, 11.37 batches/s, loss=2.574/1.925, train accuracy=78.571/92.547, learning_rate=3.410e-03]\n",
      "Training Epoch 97/300: 100%|| 7/7 [00:00<00:00, 12.05 batches/s, loss=2.638/2.044, train accuracy=92.857/89.717, learning_rate=3.392e-03]\n",
      "Training Epoch 98/300: 100%|| 7/7 [00:00<00:00, 11.01 batches/s, loss=2.299/1.954, train accuracy=85.714/89.441, learning_rate=3.375e-03]\n",
      "Training Epoch 99/300: 100%|| 7/7 [00:00<00:00, 11.22 batches/s, loss=1.848/1.835, train accuracy=92.857/90.614, learning_rate=3.358e-03]\n",
      "Training Epoch 100/300: 100%|| 7/7 [00:00<00:00, 11.67 batches/s, loss=2.798/1.987, train accuracy=57.143/91.649, learning_rate=3.341e-03]\n",
      "Training Epoch 101/300: 100%|| 7/7 [00:00<00:00, 10.79 batches/s, loss=3.686/2.168, train accuracy=78.571/89.786, learning_rate=3.324e-03]\n",
      "Training Epoch 102/300: 100%|| 7/7 [00:00<00:00, 11.36 batches/s, loss=3.313/2.135, train accuracy=71.429/88.613, learning_rate=3.308e-03]\n",
      "Training Epoch 103/300: 100%|| 7/7 [00:00<00:00, 10.83 batches/s, loss=2.802/2.083, train accuracy=78.571/87.164, learning_rate=3.292e-03]\n",
      "Training Epoch 104/300: 100%|| 7/7 [00:00<00:00, 11.11 batches/s, loss=2.030/1.985, train accuracy=92.857/88.406, learning_rate=3.276e-03]\n",
      "Training Epoch 105/300: 100%|| 7/7 [00:00<00:00, 11.31 batches/s, loss=1.706/1.791, train accuracy=100.000/92.133, learning_rate=3.260e-03]\n",
      "Training Epoch 106/300: 100%|| 7/7 [00:00<00:00, 10.88 batches/s, loss=2.044/1.779, train accuracy=85.714/93.927, learning_rate=3.245e-03]\n",
      "Training Epoch 107/300: 100%|| 7/7 [00:00<00:00, 11.56 batches/s, loss=1.738/1.707, train accuracy=100.000/95.169, learning_rate=3.230e-03]\n",
      "Training Epoch 108/300: 100%|| 7/7 [00:00<00:00, 11.19 batches/s, loss=1.718/1.660, train accuracy=92.857/96.204, learning_rate=3.215e-03]\n",
      "Training Epoch 109/300: 100%|| 7/7 [00:00<00:00, 11.20 batches/s, loss=2.014/1.695, train accuracy=85.714/96.825, learning_rate=3.200e-03]\n",
      "Training Epoch 110/300: 100%|| 7/7 [00:00<00:00, 11.25 batches/s, loss=2.179/1.713, train accuracy=85.714/96.963, learning_rate=3.185e-03]\n",
      "Training Epoch 111/300: 100%|| 7/7 [00:00<00:00, 10.71 batches/s, loss=2.254/1.726, train accuracy=92.857/97.170, learning_rate=3.171e-03]\n",
      "Training Epoch 112/300: 100%|| 7/7 [00:00<00:00, 11.72 batches/s, loss=1.932/1.644, train accuracy=78.571/97.585, learning_rate=3.157e-03]\n",
      "Training Epoch 113/300: 100%|| 7/7 [00:00<00:00, 10.70 batches/s, loss=4.477/2.071, train accuracy=85.714/95.790, learning_rate=3.143e-03]\n",
      "Training Epoch 114/300: 100%|| 7/7 [00:00<00:00, 11.00 batches/s, loss=4.834/2.113, train accuracy=50.000/95.721, learning_rate=3.129e-03]\n",
      "Training Epoch 115/300: 100%|| 7/7 [00:00<00:00, 10.91 batches/s, loss=2.327/1.795, train accuracy=85.714/94.755, learning_rate=3.115e-03]\n",
      "Training Epoch 116/300: 100%|| 7/7 [00:00<00:00, 12.26 batches/s, loss=2.306/1.771, train accuracy=78.571/96.135, learning_rate=3.102e-03]\n",
      "Training Epoch 117/300: 100%|| 7/7 [00:00<00:00, 12.07 batches/s, loss=1.971/1.911, train accuracy=78.571/87.716, learning_rate=3.089e-03]\n",
      "Training Epoch 118/300: 100%|| 7/7 [00:00<00:00, 10.98 batches/s, loss=2.537/1.865, train accuracy=78.571/93.168, learning_rate=3.075e-03]\n",
      "Training Epoch 119/300: 100%|| 7/7 [00:00<00:00, 11.92 batches/s, loss=3.787/2.045, train accuracy=42.857/92.823, learning_rate=3.062e-03]\n",
      "Training Epoch 120/300: 100%|| 7/7 [00:00<00:00, 11.02 batches/s, loss=3.825/2.152, train accuracy=57.143/89.786, learning_rate=3.050e-03]\n",
      "Training Epoch 121/300: 100%|| 7/7 [00:00<00:00, 11.55 batches/s, loss=3.444/2.172, train accuracy=78.571/87.233, learning_rate=3.037e-03]\n",
      "Training Epoch 122/300: 100%|| 7/7 [00:00<00:00, 11.13 batches/s, loss=3.141/2.030, train accuracy=64.286/88.958, learning_rate=3.025e-03]\n",
      "Training Epoch 123/300: 100%|| 7/7 [00:00<00:00, 11.91 batches/s, loss=4.035/2.199, train accuracy=57.143/88.820, learning_rate=3.012e-03]\n",
      "Training Epoch 124/300: 100%|| 7/7 [00:00<00:00, 12.79 batches/s, loss=2.689/1.957, train accuracy=71.429/90.752, learning_rate=3.000e-03]\n",
      "Training Epoch 125/300: 100%|| 7/7 [00:00<00:00, 11.59 batches/s, loss=2.688/1.944, train accuracy=78.571/91.028, learning_rate=2.988e-03]\n",
      "Training Epoch 126/300: 100%|| 7/7 [00:00<00:00, 11.70 batches/s, loss=2.977/1.885, train accuracy=64.286/93.651, learning_rate=2.976e-03]\n",
      "Training Epoch 127/300: 100%|| 7/7 [00:00<00:00, 11.48 batches/s, loss=3.212/1.880, train accuracy=85.714/95.307, learning_rate=2.964e-03]\n",
      "Training Epoch 128/300: 100%|| 7/7 [00:00<00:00, 11.18 batches/s, loss=1.875/1.661, train accuracy=85.714/95.100, learning_rate=2.953e-03]\n",
      "Training Epoch 129/300: 100%|| 7/7 [00:00<00:00, 11.30 batches/s, loss=1.585/1.584, train accuracy=92.857/97.032, learning_rate=2.941e-03]\n",
      "Training Epoch 130/300: 100%|| 7/7 [00:00<00:00, 11.43 batches/s, loss=1.545/1.541, train accuracy=100.000/97.930, learning_rate=2.930e-03]\n",
      "Training Epoch 131/300: 100%|| 7/7 [00:00<00:00, 11.21 batches/s, loss=1.415/1.479, train accuracy=100.000/99.517, learning_rate=2.919e-03]\n",
      "Training Epoch 132/300: 100%|| 7/7 [00:00<00:00, 12.09 batches/s, loss=1.354/1.449, train accuracy=100.000/99.448, learning_rate=2.908e-03]\n",
      "Training Epoch 133/300: 100%|| 7/7 [00:00<00:00, 11.82 batches/s, loss=1.334/1.427, train accuracy=100.000/99.586, learning_rate=2.897e-03]\n",
      "Training Epoch 134/300: 100%|| 7/7 [00:00<00:00, 11.16 batches/s, loss=1.306/1.422, train accuracy=100.000/99.586, learning_rate=2.886e-03]\n",
      "Training Epoch 135/300: 100%|| 7/7 [00:00<00:00, 10.67 batches/s, loss=1.313/1.410, train accuracy=100.000/99.586, learning_rate=2.875e-03]\n",
      "Training Epoch 136/300: 100%|| 7/7 [00:00<00:00, 10.39 batches/s, loss=1.324/1.395, train accuracy=100.000/99.448, learning_rate=2.865e-03] \n",
      "Training Epoch 137/300: 100%|| 7/7 [00:00<00:00, 10.85 batches/s, loss=1.294/1.377, train accuracy=100.000/99.724, learning_rate=2.854e-03]\n",
      "Training Epoch 138/300: 100%|| 7/7 [00:00<00:00, 10.47 batches/s, loss=1.313/1.378, train accuracy=100.000/99.655, learning_rate=2.844e-03]\n",
      "Training Epoch 139/300: 100%|| 7/7 [00:00<00:00, 11.03 batches/s, loss=1.320/1.371, train accuracy=100.000/99.793, learning_rate=2.834e-03] \n",
      "Training Epoch 140/300: 100%|| 7/7 [00:00<00:00, 11.46 batches/s, loss=1.306/1.368, train accuracy=100.000/99.862, learning_rate=2.823e-03]\n",
      "Training Epoch 141/300: 100%|| 7/7 [00:00<00:00, 10.64 batches/s, loss=1.340/1.381, train accuracy=100.000/99.655, learning_rate=2.813e-03]\n",
      "Training Epoch 142/300: 100%|| 7/7 [00:00<00:00, 12.20 batches/s, loss=1.361/1.377, train accuracy=100.000/99.724, learning_rate=2.804e-03]\n",
      "Training Epoch 143/300: 100%|| 7/7 [00:00<00:00, 11.52 batches/s, loss=1.307/1.364, train accuracy=100.000/99.586, learning_rate=2.794e-03]\n",
      "Training Epoch 144/300: 100%|| 7/7 [00:00<00:00, 11.14 batches/s, loss=1.349/1.371, train accuracy=100.000/99.586, learning_rate=2.784e-03] \n",
      "Training Epoch 145/300: 100%|| 7/7 [00:00<00:00, 11.93 batches/s, loss=1.354/1.365, train accuracy=100.000/99.724, learning_rate=2.774e-03]\n",
      "Training Epoch 146/300: 100%|| 7/7 [00:00<00:00, 11.23 batches/s, loss=1.299/1.363, train accuracy=100.000/99.655, learning_rate=2.765e-03]\n",
      "Training Epoch 147/300: 100%|| 7/7 [00:00<00:00, 11.37 batches/s, loss=1.352/1.370, train accuracy=100.000/99.655, learning_rate=2.755e-03]\n",
      "Training Epoch 148/300: 100%|| 7/7 [00:00<00:00, 10.83 batches/s, loss=1.582/1.413, train accuracy=92.857/99.724, learning_rate=2.746e-03] \n",
      "Training Epoch 149/300: 100%|| 7/7 [00:00<00:00, 10.99 batches/s, loss=1.485/1.415, train accuracy=92.857/99.448, learning_rate=2.737e-03] \n",
      "Training Epoch 150/300: 100%|| 7/7 [00:00<00:00, 11.77 batches/s, loss=2.006/1.499, train accuracy=92.857/99.310, learning_rate=2.728e-03]\n",
      "Training Epoch 151/300: 100%|| 7/7 [00:00<00:00, 11.67 batches/s, loss=1.980/1.530, train accuracy=85.714/98.482, learning_rate=2.719e-03]\n",
      "Training Epoch 152/300: 100%|| 7/7 [00:00<00:00, 10.74 batches/s, loss=3.024/1.742, train accuracy=85.714/98.137, learning_rate=2.710e-03]\n",
      "Training Epoch 153/300: 100%|| 7/7 [00:00<00:00, 10.93 batches/s, loss=3.288/1.827, train accuracy=64.286/96.411, learning_rate=2.701e-03]\n",
      "Training Epoch 154/300: 100%|| 7/7 [00:00<00:00, 11.56 batches/s, loss=2.805/1.755, train accuracy=71.429/96.549, learning_rate=2.692e-03]\n",
      "Training Epoch 155/300: 100%|| 7/7 [00:00<00:00, 12.26 batches/s, loss=3.998/1.975, train accuracy=71.429/96.135, learning_rate=2.683e-03]\n",
      "Training Epoch 156/300: 100%|| 7/7 [00:00<00:00, 11.28 batches/s, loss=2.818/1.782, train accuracy=71.429/96.135, learning_rate=2.675e-03]\n",
      "Training Epoch 157/300: 100%|| 7/7 [00:00<00:00, 11.19 batches/s, loss=2.560/1.770, train accuracy=78.571/95.376, learning_rate=2.666e-03]\n",
      "Training Epoch 158/300: 100%|| 7/7 [00:00<00:00, 11.07 batches/s, loss=2.717/1.774, train accuracy=78.571/95.307, learning_rate=2.658e-03]\n",
      "Training Epoch 159/300: 100%|| 7/7 [00:00<00:00, 10.86 batches/s, loss=2.798/1.798, train accuracy=71.429/95.445, learning_rate=2.649e-03]\n",
      "Training Epoch 160/300: 100%|| 7/7 [00:00<00:00, 12.19 batches/s, loss=3.355/1.886, train accuracy=64.286/95.307, learning_rate=2.641e-03]\n",
      "Training Epoch 161/300: 100%|| 7/7 [00:00<00:00, 11.58 batches/s, loss=3.085/1.937, train accuracy=71.429/94.065, learning_rate=2.633e-03]\n",
      "Training Epoch 162/300: 100%|| 7/7 [00:00<00:00, 11.78 batches/s, loss=2.367/1.791, train accuracy=85.714/93.306, learning_rate=2.625e-03]\n",
      "Training Epoch 163/300: 100%|| 7/7 [00:00<00:00, 11.20 batches/s, loss=2.542/1.869, train accuracy=71.429/91.166, learning_rate=2.617e-03]\n",
      "Training Epoch 164/300: 100%|| 7/7 [00:00<00:00, 11.34 batches/s, loss=1.818/1.690, train accuracy=92.857/94.755, learning_rate=2.609e-03]\n",
      "Training Epoch 165/300: 100%|| 7/7 [00:00<00:00, 10.80 batches/s, loss=2.731/1.747, train accuracy=78.571/95.583, learning_rate=2.601e-03]\n",
      "Training Epoch 166/300: 100%|| 7/7 [00:00<00:00, 11.65 batches/s, loss=3.031/1.784, train accuracy=85.714/95.928, learning_rate=2.593e-03]\n",
      "Training Epoch 167/300: 100%|| 7/7 [00:00<00:00, 10.58 batches/s, loss=4.521/1.955, train accuracy=35.714/97.239, learning_rate=2.585e-03]\n",
      "Training Epoch 168/300: 100%|| 7/7 [00:00<00:00, 11.07 batches/s, loss=3.075/1.789, train accuracy=64.286/96.894, learning_rate=2.577e-03]\n",
      "Training Epoch 169/300: 100%|| 7/7 [00:00<00:00, 11.27 batches/s, loss=2.750/1.789, train accuracy=71.429/95.859, learning_rate=2.570e-03]\n",
      "Training Epoch 170/300: 100%|| 7/7 [00:00<00:00, 11.46 batches/s, loss=3.358/1.956, train accuracy=71.429/93.099, learning_rate=2.562e-03]\n",
      "Training Epoch 171/300: 100%|| 7/7 [00:00<00:00, 10.32 batches/s, loss=2.951/1.888, train accuracy=71.429/93.099, learning_rate=2.555e-03]\n",
      "Training Epoch 172/300: 100%|| 7/7 [00:00<00:00, 11.49 batches/s, loss=2.101/1.714, train accuracy=85.714/94.272, learning_rate=2.547e-03]\n",
      "Training Epoch 173/300: 100%|| 7/7 [00:00<00:00, 11.22 batches/s, loss=1.712/1.662, train accuracy=92.857/94.272, learning_rate=2.540e-03]\n",
      "Training Epoch 174/300: 100%|| 7/7 [00:00<00:00, 10.80 batches/s, loss=1.970/1.649, train accuracy=85.714/96.411, learning_rate=2.533e-03]\n",
      "Training Epoch 175/300: 100%|| 7/7 [00:00<00:00, 11.01 batches/s, loss=2.006/1.595, train accuracy=85.714/97.170, learning_rate=2.525e-03]\n",
      "Training Epoch 176/300: 100%|| 7/7 [00:00<00:00, 10.98 batches/s, loss=1.533/1.549, train accuracy=100.000/96.549, learning_rate=2.518e-03]\n",
      "Training Epoch 177/300: 100%|| 7/7 [00:00<00:00, 10.79 batches/s, loss=1.437/1.547, train accuracy=100.000/96.756, learning_rate=2.511e-03]\n",
      "Training Epoch 178/300: 100%|| 7/7 [00:00<00:00, 11.29 batches/s, loss=1.568/1.497, train accuracy=92.857/98.068, learning_rate=2.504e-03]\n",
      "Training Epoch 179/300: 100%|| 7/7 [00:00<00:00, 11.46 batches/s, loss=1.935/1.542, train accuracy=92.857/97.999, learning_rate=2.497e-03]\n",
      "Training Epoch 180/300: 100%|| 7/7 [00:00<00:00, 11.75 batches/s, loss=1.406/1.461, train accuracy=100.000/98.896, learning_rate=2.490e-03]\n",
      "Training Epoch 181/300: 100%|| 7/7 [00:00<00:00, 11.75 batches/s, loss=1.388/1.437, train accuracy=100.000/99.241, learning_rate=2.483e-03]\n",
      "Training Epoch 182/300: 100%|| 7/7 [00:00<00:00, 10.97 batches/s, loss=1.736/1.455, train accuracy=92.857/99.310, learning_rate=2.476e-03] \n",
      "Training Epoch 183/300: 100%|| 7/7 [00:00<00:00, 11.25 batches/s, loss=1.336/1.387, train accuracy=100.000/99.379, learning_rate=2.470e-03]\n",
      "Training Epoch 184/300: 100%|| 7/7 [00:00<00:00, 12.96 batches/s, loss=1.325/1.377, train accuracy=100.000/99.586, learning_rate=2.463e-03]\n",
      "Training Epoch 185/300: 100%|| 7/7 [00:00<00:00, 11.43 batches/s, loss=1.297/1.361, train accuracy=100.000/99.793, learning_rate=2.456e-03] \n",
      "Training Epoch 186/300: 100%|| 7/7 [00:00<00:00, 11.28 batches/s, loss=1.288/1.350, train accuracy=100.000/99.862, learning_rate=2.450e-03] \n",
      "Training Epoch 187/300: 100%|| 7/7 [00:00<00:00, 11.28 batches/s, loss=1.279/1.346, train accuracy=100.000/99.724, learning_rate=2.443e-03]\n",
      "Training Epoch 188/300: 100%|| 7/7 [00:00<00:00, 11.84 batches/s, loss=1.274/1.345, train accuracy=100.000/99.655, learning_rate=2.437e-03]\n",
      "Training Epoch 189/300: 100%|| 7/7 [00:00<00:00, 12.39 batches/s, loss=1.274/1.341, train accuracy=100.000/99.724, learning_rate=2.430e-03]\n",
      "Training Epoch 190/300: 100%|| 7/7 [00:00<00:00, 11.61 batches/s, loss=1.272/1.336, train accuracy=100.000/99.724, learning_rate=2.424e-03]\n",
      "Training Epoch 191/300: 100%|| 7/7 [00:00<00:00, 11.07 batches/s, loss=1.272/1.326, train accuracy=100.000/99.724, learning_rate=2.417e-03]\n",
      "Training Epoch 192/300: 100%|| 7/7 [00:00<00:00, 10.83 batches/s, loss=1.270/1.326, train accuracy=100.000/99.862, learning_rate=2.411e-03]\n",
      "Training Epoch 193/300: 100%|| 7/7 [00:00<00:00, 11.22 batches/s, loss=1.272/1.326, train accuracy=100.000/99.862, learning_rate=2.405e-03] \n",
      "Training Epoch 194/300: 100%|| 7/7 [00:00<00:00, 11.74 batches/s, loss=1.272/1.327, train accuracy=100.000/99.793, learning_rate=2.399e-03]\n",
      "Training Epoch 195/300: 100%|| 7/7 [00:00<00:00, 11.35 batches/s, loss=1.269/1.335, train accuracy=100.000/99.655, learning_rate=2.392e-03]\n",
      "Training Epoch 196/300: 100%|| 7/7 [00:00<00:00, 12.11 batches/s, loss=1.287/1.344, train accuracy=100.000/99.793, learning_rate=2.386e-03]\n",
      "Training Epoch 197/300: 100%|| 7/7 [00:00<00:00, 11.52 batches/s, loss=1.319/1.343, train accuracy=100.000/99.655, learning_rate=2.380e-03]\n",
      "Training Epoch 198/300: 100%|| 7/7 [00:00<00:00, 11.46 batches/s, loss=1.294/1.335, train accuracy=100.000/99.655, learning_rate=2.374e-03]\n",
      "Training Epoch 199/300: 100%|| 7/7 [00:00<00:00, 10.95 batches/s, loss=1.302/1.352, train accuracy=100.000/99.448, learning_rate=2.368e-03]\n",
      "Training Epoch 200/300: 100%|| 7/7 [00:00<00:00, 10.95 batches/s, loss=1.281/1.349, train accuracy=100.000/99.586, learning_rate=2.362e-03]\n",
      "Training Epoch 201/300: 100%|| 7/7 [00:00<00:00, 11.71 batches/s, loss=1.281/1.345, train accuracy=100.000/99.586, learning_rate=2.356e-03]\n",
      "Training Epoch 202/300: 100%|| 7/7 [00:00<00:00, 11.12 batches/s, loss=1.291/1.353, train accuracy=100.000/99.379, learning_rate=2.351e-03]\n",
      "Training Epoch 203/300: 100%|| 7/7 [00:00<00:00, 11.18 batches/s, loss=1.286/1.354, train accuracy=100.000/99.172, learning_rate=2.345e-03]\n",
      "Training Epoch 204/300: 100%|| 7/7 [00:00<00:00, 11.59 batches/s, loss=1.292/1.356, train accuracy=100.000/99.310, learning_rate=2.339e-03]\n",
      "Training Epoch 205/300: 100%|| 7/7 [00:00<00:00, 11.19 batches/s, loss=1.349/1.368, train accuracy=100.000/99.241, learning_rate=2.333e-03]\n",
      "Training Epoch 206/300: 100%|| 7/7 [00:00<00:00, 10.80 batches/s, loss=1.356/1.396, train accuracy=100.000/98.551, learning_rate=2.328e-03]\n",
      "Training Epoch 207/300: 100%|| 7/7 [00:00<00:00, 11.48 batches/s, loss=1.339/1.452, train accuracy=100.000/97.792, learning_rate=2.322e-03]\n",
      "Training Epoch 208/300: 100%|| 7/7 [00:00<00:00, 11.05 batches/s, loss=1.350/1.428, train accuracy=100.000/97.930, learning_rate=2.316e-03]\n",
      "Training Epoch 209/300: 100%|| 7/7 [00:00<00:00, 11.60 batches/s, loss=1.422/1.438, train accuracy=92.857/98.137, learning_rate=2.311e-03]\n",
      "Training Epoch 210/300: 100%|| 7/7 [00:00<00:00, 10.80 batches/s, loss=1.338/1.435, train accuracy=100.000/98.206, learning_rate=2.305e-03]\n",
      "Training Epoch 211/300: 100%|| 7/7 [00:00<00:00, 11.77 batches/s, loss=1.359/1.428, train accuracy=100.000/98.620, learning_rate=2.300e-03]\n",
      "Training Epoch 212/300: 100%|| 7/7 [00:00<00:00, 10.95 batches/s, loss=1.311/1.402, train accuracy=100.000/98.689, learning_rate=2.294e-03]\n",
      "Training Epoch 213/300: 100%|| 7/7 [00:00<00:00, 11.84 batches/s, loss=1.319/1.409, train accuracy=100.000/98.689, learning_rate=2.289e-03]\n",
      "Training Epoch 214/300: 100%|| 7/7 [00:00<00:00, 11.50 batches/s, loss=1.327/1.418, train accuracy=100.000/98.689, learning_rate=2.284e-03]\n",
      "Training Epoch 215/300: 100%|| 7/7 [00:00<00:00, 11.76 batches/s, loss=1.333/1.448, train accuracy=100.000/98.620, learning_rate=2.278e-03]\n",
      "Training Epoch 216/300: 100%|| 7/7 [00:00<00:00, 10.41 batches/s, loss=1.319/1.411, train accuracy=100.000/99.310, learning_rate=2.273e-03]\n",
      "Training Epoch 217/300: 100%|| 7/7 [00:00<00:00, 11.27 batches/s, loss=1.460/1.415, train accuracy=92.857/99.241, learning_rate=2.268e-03] \n",
      "Training Epoch 218/300: 100%|| 7/7 [00:00<00:00, 10.97 batches/s, loss=1.428/1.411, train accuracy=92.857/99.103, learning_rate=2.263e-03]\n",
      "Training Epoch 219/300: 100%|| 7/7 [00:00<00:00, 12.01 batches/s, loss=1.562/1.434, train accuracy=100.000/99.241, learning_rate=2.257e-03]\n",
      "Training Epoch 220/300: 100%|| 7/7 [00:00<00:00, 11.09 batches/s, loss=1.407/1.471, train accuracy=100.000/97.654, learning_rate=2.252e-03]\n",
      "Training Epoch 221/300: 100%|| 7/7 [00:00<00:00, 10.95 batches/s, loss=1.711/1.478, train accuracy=92.857/98.620, learning_rate=2.247e-03] \n",
      "Training Epoch 222/300: 100%|| 7/7 [00:00<00:00, 11.24 batches/s, loss=1.478/1.448, train accuracy=100.000/97.792, learning_rate=2.242e-03]\n",
      "Training Epoch 223/300: 100%|| 7/7 [00:00<00:00, 10.86 batches/s, loss=1.524/1.436, train accuracy=100.000/98.413, learning_rate=2.237e-03]\n",
      "Training Epoch 224/300: 100%|| 7/7 [00:00<00:00, 11.72 batches/s, loss=1.559/1.429, train accuracy=100.000/98.620, learning_rate=2.232e-03]\n",
      "Training Epoch 225/300: 100%|| 7/7 [00:00<00:00, 12.11 batches/s, loss=1.514/1.418, train accuracy=100.000/98.827, learning_rate=2.227e-03]\n",
      "Training Epoch 226/300: 100%|| 7/7 [00:00<00:00, 10.90 batches/s, loss=1.366/1.388, train accuracy=100.000/98.896, learning_rate=2.222e-03]\n",
      "Training Epoch 227/300: 100%|| 7/7 [00:00<00:00, 10.88 batches/s, loss=1.362/1.366, train accuracy=100.000/99.310, learning_rate=2.217e-03]\n",
      "Training Epoch 228/300: 100%|| 7/7 [00:00<00:00, 10.52 batches/s, loss=1.346/1.366, train accuracy=100.000/99.655, learning_rate=2.212e-03] \n",
      "Training Epoch 229/300: 100%|| 7/7 [00:00<00:00, 10.86 batches/s, loss=1.317/1.353, train accuracy=100.000/99.310, learning_rate=2.208e-03]\n",
      "Training Epoch 230/300: 100%|| 7/7 [00:00<00:00, 10.97 batches/s, loss=1.271/1.337, train accuracy=100.000/99.448, learning_rate=2.203e-03]\n",
      "Training Epoch 231/300: 100%|| 7/7 [00:00<00:00, 11.28 batches/s, loss=1.270/1.333, train accuracy=100.000/99.448, learning_rate=2.198e-03]\n",
      "Training Epoch 232/300: 100%|| 7/7 [00:00<00:00, 11.27 batches/s, loss=1.267/1.324, train accuracy=100.000/99.655, learning_rate=2.193e-03]\n",
      "Training Epoch 233/300: 100%|| 7/7 [00:00<00:00, 10.46 batches/s, loss=1.274/1.323, train accuracy=100.000/99.655, learning_rate=2.189e-03] \n",
      "Training Epoch 234/300: 100%|| 7/7 [00:00<00:00, 11.38 batches/s, loss=1.260/1.314, train accuracy=100.000/99.586, learning_rate=2.184e-03] \n",
      "Training Epoch 235/300: 100%|| 7/7 [00:00<00:00, 11.32 batches/s, loss=1.270/1.318, train accuracy=100.000/99.724, learning_rate=2.179e-03] \n",
      "Training Epoch 236/300: 100%|| 7/7 [00:00<00:00, 12.44 batches/s, loss=1.281/1.316, train accuracy=100.000/99.655, learning_rate=2.175e-03]\n",
      "Training Epoch 237/300: 100%|| 7/7 [00:00<00:00, 11.04 batches/s, loss=1.276/1.311, train accuracy=100.000/99.793, learning_rate=2.170e-03]\n",
      "Training Epoch 238/300: 100%|| 7/7 [00:00<00:00, 11.02 batches/s, loss=1.282/1.317, train accuracy=100.000/99.862, learning_rate=2.165e-03] \n",
      "Training Epoch 239/300: 100%|| 7/7 [00:00<00:00, 11.43 batches/s, loss=1.267/1.317, train accuracy=100.000/99.655, learning_rate=2.161e-03]\n",
      "Training Epoch 240/300: 100%|| 7/7 [00:00<00:00, 11.23 batches/s, loss=1.294/1.322, train accuracy=100.000/99.655, learning_rate=2.156e-03]\n",
      "Training Epoch 241/300: 100%|| 7/7 [00:00<00:00, 11.96 batches/s, loss=1.352/1.332, train accuracy=100.000/99.586, learning_rate=2.152e-03]\n",
      "Training Epoch 242/300: 100%|| 7/7 [00:00<00:00, 10.63 batches/s, loss=1.327/1.338, train accuracy=100.000/99.586, learning_rate=2.148e-03] \n",
      "Training Epoch 243/300: 100%|| 7/7 [00:00<00:00, 10.94 batches/s, loss=1.318/1.329, train accuracy=100.000/99.517, learning_rate=2.143e-03]\n",
      "Training Epoch 244/300: 100%|| 7/7 [00:00<00:00, 11.05 batches/s, loss=1.303/1.323, train accuracy=100.000/99.586, learning_rate=2.139e-03]\n",
      "Training Epoch 245/300: 100%|| 7/7 [00:00<00:00, 10.60 batches/s, loss=1.290/1.325, train accuracy=100.000/99.517, learning_rate=2.134e-03]\n",
      "Training Epoch 246/300: 100%|| 7/7 [00:00<00:00, 11.00 batches/s, loss=1.283/1.318, train accuracy=100.000/99.586, learning_rate=2.130e-03]\n",
      "Training Epoch 247/300: 100%|| 7/7 [00:00<00:00, 10.92 batches/s, loss=1.266/1.325, train accuracy=100.000/99.310, learning_rate=2.126e-03]\n",
      "Training Epoch 248/300: 100%|| 7/7 [00:00<00:00, 11.77 batches/s, loss=1.273/1.348, train accuracy=100.000/99.034, learning_rate=2.121e-03]\n",
      "Training Epoch 249/300: 100%|| 7/7 [00:00<00:00, 11.12 batches/s, loss=1.291/1.348, train accuracy=100.000/99.448, learning_rate=2.117e-03]\n",
      "Training Epoch 250/300: 100%|| 7/7 [00:00<00:00, 11.27 batches/s, loss=1.324/1.357, train accuracy=100.000/99.172, learning_rate=2.113e-03] \n",
      "Training Epoch 251/300: 100%|| 7/7 [00:00<00:00, 11.16 batches/s, loss=1.307/1.354, train accuracy=100.000/99.034, learning_rate=2.109e-03]\n",
      "Training Epoch 252/300: 100%|| 7/7 [00:00<00:00, 11.54 batches/s, loss=1.346/1.363, train accuracy=100.000/99.103, learning_rate=2.104e-03]\n",
      "Training Epoch 253/300: 100%|| 7/7 [00:00<00:00, 10.71 batches/s, loss=1.294/1.360, train accuracy=100.000/98.758, learning_rate=2.100e-03]\n",
      "Training Epoch 254/300: 100%|| 7/7 [00:00<00:00, 10.82 batches/s, loss=1.297/1.360, train accuracy=100.000/98.689, learning_rate=2.096e-03]\n",
      "Training Epoch 255/300: 100%|| 7/7 [00:00<00:00, 11.05 batches/s, loss=1.322/1.376, train accuracy=100.000/98.413, learning_rate=2.092e-03]\n",
      "Training Epoch 256/300: 100%|| 7/7 [00:00<00:00, 11.53 batches/s, loss=1.293/1.372, train accuracy=100.000/98.482, learning_rate=2.088e-03]\n",
      "Training Epoch 257/300: 100%|| 7/7 [00:00<00:00, 11.03 batches/s, loss=1.287/1.360, train accuracy=100.000/99.034, learning_rate=2.084e-03] \n",
      "Training Epoch 258/300: 100%|| 7/7 [00:00<00:00, 11.65 batches/s, loss=1.272/1.387, train accuracy=100.000/98.344, learning_rate=2.080e-03]\n",
      "Training Epoch 259/300: 100%|| 7/7 [00:00<00:00, 11.24 batches/s, loss=1.260/1.357, train accuracy=100.000/99.103, learning_rate=2.076e-03]\n",
      "Training Epoch 260/300: 100%|| 7/7 [00:00<00:00, 11.68 batches/s, loss=1.272/1.355, train accuracy=100.000/98.896, learning_rate=2.072e-03] \n",
      "Training Epoch 261/300: 100%|| 7/7 [00:00<00:00, 10.74 batches/s, loss=1.285/1.369, train accuracy=100.000/98.758, learning_rate=2.068e-03]\n",
      "Training Epoch 262/300: 100%|| 7/7 [00:00<00:00, 10.95 batches/s, loss=1.584/1.443, train accuracy=92.857/98.482, learning_rate=2.064e-03]\n",
      "Training Epoch 263/300: 100%|| 7/7 [00:00<00:00, 10.90 batches/s, loss=1.305/1.381, train accuracy=100.000/98.896, learning_rate=2.060e-03]\n",
      "Training Epoch 264/300: 100%|| 7/7 [00:00<00:00, 11.47 batches/s, loss=1.302/1.381, train accuracy=100.000/98.758, learning_rate=2.056e-03]\n",
      "Training Epoch 265/300: 100%|| 7/7 [00:00<00:00, 11.67 batches/s, loss=1.290/1.360, train accuracy=100.000/98.620, learning_rate=2.052e-03]\n",
      "Training Epoch 266/300: 100%|| 7/7 [00:00<00:00, 10.92 batches/s, loss=1.290/1.368, train accuracy=100.000/98.689, learning_rate=2.048e-03]\n",
      "Training Epoch 267/300: 100%|| 7/7 [00:00<00:00, 11.50 batches/s, loss=1.357/1.349, train accuracy=92.857/99.310, learning_rate=2.045e-03] \n",
      "Training Epoch 268/300: 100%|| 7/7 [00:00<00:00, 10.93 batches/s, loss=1.291/1.327, train accuracy=100.000/99.517, learning_rate=2.041e-03] \n",
      "Training Epoch 269/300: 100%|| 7/7 [00:00<00:00, 11.32 batches/s, loss=1.268/1.323, train accuracy=100.000/99.448, learning_rate=2.037e-03]\n",
      "Training Epoch 270/300: 100%|| 7/7 [00:00<00:00, 12.26 batches/s, loss=1.278/1.327, train accuracy=100.000/99.724, learning_rate=2.033e-03]\n",
      "Training Epoch 271/300: 100%|| 7/7 [00:00<00:00, 11.43 batches/s, loss=1.307/1.328, train accuracy=100.000/99.586, learning_rate=2.029e-03]\n",
      "Training Epoch 272/300: 100%|| 7/7 [00:00<00:00, 11.06 batches/s, loss=1.273/1.311, train accuracy=100.000/99.862, learning_rate=2.026e-03] \n",
      "Training Epoch 273/300: 100%|| 7/7 [00:00<00:00, 11.79 batches/s, loss=1.273/1.306, train accuracy=100.000/99.586, learning_rate=2.022e-03] \n",
      "Training Epoch 274/300: 100%|| 7/7 [00:00<00:00, 12.17 batches/s, loss=1.272/1.301, train accuracy=100.000/99.793, learning_rate=2.018e-03] \n",
      "Training Epoch 275/300: 100%|| 7/7 [00:00<00:00, 10.63 batches/s, loss=1.259/1.299, train accuracy=100.000/99.793, learning_rate=2.015e-03] \n",
      "Training Epoch 276/300: 100%|| 7/7 [00:00<00:00, 11.12 batches/s, loss=1.269/1.299, train accuracy=100.000/99.655, learning_rate=2.011e-03] \n",
      "Training Epoch 277/300: 100%|| 7/7 [00:00<00:00, 11.40 batches/s, loss=1.288/1.296, train accuracy=100.000/99.724, learning_rate=2.007e-03] \n",
      "Training Epoch 278/300: 100%|| 7/7 [00:00<00:00, 11.31 batches/s, loss=1.259/1.295, train accuracy=100.000/99.862, learning_rate=2.004e-03] \n",
      "Training Epoch 279/300: 100%|| 7/7 [00:00<00:00, 11.44 batches/s, loss=1.272/1.312, train accuracy=100.000/99.655, learning_rate=2.000e-03] \n",
      "Training Epoch 280/300: 100%|| 7/7 [00:00<00:00, 10.56 batches/s, loss=1.286/1.314, train accuracy=100.000/99.517, learning_rate=1.996e-03]\n",
      "Training Epoch 281/300: 100%|| 7/7 [00:00<00:00, 11.16 batches/s, loss=1.258/1.309, train accuracy=100.000/99.448, learning_rate=1.993e-03] \n",
      "Training Epoch 282/300: 100%|| 7/7 [00:00<00:00, 11.73 batches/s, loss=1.277/1.322, train accuracy=100.000/99.241, learning_rate=1.989e-03]\n",
      "Training Epoch 283/300: 100%|| 7/7 [00:00<00:00, 12.49 batches/s, loss=1.291/1.322, train accuracy=100.000/99.241, learning_rate=1.986e-03]\n",
      "Training Epoch 284/300: 100%|| 7/7 [00:00<00:00, 12.38 batches/s, loss=1.274/1.312, train accuracy=100.000/99.655, learning_rate=1.982e-03]\n",
      "Training Epoch 285/300: 100%|| 7/7 [00:00<00:00, 12.01 batches/s, loss=1.284/1.321, train accuracy=100.000/99.586, learning_rate=1.979e-03]\n",
      "Training Epoch 286/300: 100%|| 7/7 [00:00<00:00, 11.55 batches/s, loss=1.263/1.332, train accuracy=100.000/99.448, learning_rate=1.975e-03]\n",
      "Training Epoch 287/300: 100%|| 7/7 [00:00<00:00, 10.80 batches/s, loss=1.421/1.339, train accuracy=92.857/99.310, learning_rate=1.972e-03]  \n",
      "Training Epoch 288/300: 100%|| 7/7 [00:00<00:00, 11.18 batches/s, loss=1.301/1.343, train accuracy=100.000/99.517, learning_rate=1.969e-03]\n",
      "Training Epoch 289/300: 100%|| 7/7 [00:00<00:00, 10.88 batches/s, loss=1.295/1.314, train accuracy=100.000/99.586, learning_rate=1.965e-03] \n",
      "Training Epoch 290/300: 100%|| 7/7 [00:00<00:00, 11.15 batches/s, loss=1.305/1.316, train accuracy=100.000/99.517, learning_rate=1.962e-03]\n",
      "Training Epoch 291/300: 100%|| 7/7 [00:00<00:00, 11.63 batches/s, loss=1.296/1.318, train accuracy=100.000/99.448, learning_rate=1.958e-03]\n",
      "Training Epoch 292/300: 100%|| 7/7 [00:00<00:00, 11.25 batches/s, loss=1.263/1.314, train accuracy=100.000/99.241, learning_rate=1.955e-03] \n",
      "Training Epoch 293/300: 100%|| 7/7 [00:00<00:00, 11.42 batches/s, loss=1.267/1.306, train accuracy=100.000/99.310, learning_rate=1.952e-03]\n",
      "Training Epoch 294/300: 100%|| 7/7 [00:00<00:00, 10.09 batches/s, loss=1.267/1.297, train accuracy=100.000/99.517, learning_rate=1.948e-03]\n",
      "Training Epoch 295/300: 100%|| 7/7 [00:00<00:00, 10.33 batches/s, loss=1.261/1.301, train accuracy=100.000/99.586, learning_rate=1.945e-03]\n",
      "Training Epoch 296/300: 100%|| 7/7 [00:00<00:00, 10.89 batches/s, loss=1.254/1.317, train accuracy=100.000/99.655, learning_rate=1.942e-03] \n",
      "Training Epoch 297/300: 100%|| 7/7 [00:00<00:00, 11.14 batches/s, loss=1.263/1.316, train accuracy=100.000/99.586, learning_rate=1.939e-03]\n",
      "Training Epoch 298/300: 100%|| 7/7 [00:00<00:00, 10.38 batches/s, loss=1.269/1.306, train accuracy=100.000/99.586, learning_rate=1.935e-03]\n",
      "Training Epoch 299/300: 100%|| 7/7 [00:00<00:00, 11.09 batches/s, loss=1.261/1.336, train accuracy=100.000/99.379, learning_rate=1.932e-03]\n",
      "Training Epoch 300/300: 100%|| 7/7 [00:00<00:00, 10.92 batches/s, loss=1.266/1.336, train accuracy=100.000/98.620, learning_rate=1.929e-03]\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f99929169663da66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 1/1: 100%|| 7/7 [00:00<00:00, 18.83 batches/s, loss=1.258/1.293, val accuracy=100.000/99.517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test TestModelAccuracy: \u001b[92mpassed!\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = test_and_save_model(trainer, tokenizer, submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a96e0dde07757",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "The last part to do is test your model in inference! We will feed an input sentence into the algorithm, together with a maximum number of iterations. The sentence is then tokenized, so that we can feed it into the model. The first decoder input will only be the start token, which in our model is the same as the end token! The model will give us an output distribution over all tokens in our vocabulary. We can then either choose the token with the highest probability, or we can sample from this categorical distribution! That way, every answer will be slightly different! The last output is added to the decoder sequence and on it goes! If the model predicts an end token, we stop the algorithm. Otherwise we continue until the maximum number of iterations are reached! Finally, we can decode the output. This will look something like this:\n",
    "\n",
    "\n",
    "Input Sentence: \"Hi how are you\" -> Tokenizer -> Encoder Input: [0, 45, 25, 15, 12, 0]\n",
    "\n",
    "| Iteration  | Encoder Input          | Decoder Input                | Decoder Output                                   |\n",
    "|------------|------------------------|------------------------------|--------------------------------------------------|\n",
    "| 1          | [0, 45, 25, 15, 12, 0] | [0]                          | [445]                                            |\n",
    "| 2          | [0, 45, 25, 15, 12, 0] | [0, 445]                     | [445, 56]                                        |\n",
    "| 3          | [0, 45, 25, 15, 12, 0] | [0, 445, 56]                 | [445, 56, 89]                                    |\n",
    "| 4          | [0, 45, 25, 15, 12, 0] | [0, 445, 56, 89]             | [445, 56, 89, 76]                                |\n",
    "| 5          | [0, 45, 25, 15, 12, 0] | [0, 445, 56, 89, 76]         | [445, 56, 89, 76, 0]                             | -> 0 Detected!\n",
    "\n",
    "Decoder Output: [445, 56, 89, 76, 0] -> Tokenizer -> Output Sentence: \"Hallo wie geht's dir <[EOS]>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e969f38",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 14: Check Code</h3>\n",
    "    <p>Have a look at the <code>predict()</code> method <code>Transformer</code> in <code>exercise_code/network/transformer.py</code>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b34fcc24c4a3ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence, max_iteration_length = 50, probabilistic = False, returns_scores = False):\n",
    "    # Tokenize input sentence\n",
    "    encoder_input = torch.tensor(tokenizer.encode(input_sentence))\n",
    "\n",
    "    # Retrieve output sequence from model\n",
    "    output_sequence, score_records = model.predict(encoder_input, max_iteration_length, probabilistic, returns_scores)\n",
    "\n",
    "    # Decode output sequence\n",
    "    output_sequence = tokenizer.decode(output_sequence, skip_special_tokens=True)\n",
    "    \n",
    "    if returns_scores:\n",
    "        return output_sequence, score_records\n",
    "    return output_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9856c69d65cc8f",
   "metadata": {},
   "source": [
    "Now you can test your model! Feel free to change the variable probabilistic to true! don't be to surprised, if the output is terrible at the moment though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ff81d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is on the correct device\n",
    "# Usually the Trainer does this, just in case you stopped training mid epoch!\n",
    "_ = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11848183f60a3b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo, wie geht es dir heute?\n"
     ]
    }
   ],
   "source": [
    "output_sequence = translate(\"Hi, how are you today?\", probabilistic=False)\n",
    "print(output_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3756c73bdefeaab",
   "metadata": {},
   "source": [
    "Awesome, it works! \n",
    "\n",
    "Well, sort of - This sentece was part of the dataset! Let's try it with a different sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3782e681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erst dann ist die Perspektive eines Beitritts realistisch.\n"
     ]
    }
   ],
   "source": [
    "output_sequence = translate(\"This is not part of the dataset!\", probabilistic=False)\n",
    "print(output_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781aac04",
   "metadata": {},
   "source": [
    "Well.. ups :D But remember, all we did is overfit to a small dataset with a small model!\n",
    "\n",
    "We have prepared a pretrained model for you! It was trained on a larger dataset and has a lot more parameters! You should be able to load it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91548cb2c185c862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = load_pretrained_fast()\n",
    "\n",
    "file_path = os.path.join(pretrained_model_path, 'pretrained_model')\n",
    "load_dict = torch.load(file_path, weights_only=True)\n",
    "model = Transformer(vocab_size=len(tokenizer),\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    hparams=load_dict['hparams'])\n",
    "\n",
    "model.load_state_dict(load_dict['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397a48edee3e416",
   "metadata": {},
   "source": [
    "Now try it again and see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7342c760c205ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das ist nicht Teil des Datenasets!\n"
     ]
    }
   ],
   "source": [
    "output_sequence = translate(\"This is not part of the dataset!\", probabilistic=False)\n",
    "print(output_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa9e302f00b65c",
   "metadata": {},
   "source": [
    "Alright, seems to work (Apart from a small typo, but we didn't train this model for that long)! Try out probabilistic to see some other results, they might be rubish though...\n",
    "\n",
    "Note: If your model outputs something weird, you probably made some mistake a long the way that we didn't catch! Please go over your Encoder and Decoder Blocks and make sure you did this correctly! Especially look at the residual connections!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c4a623d4aa7a8",
   "metadata": {},
   "source": [
    "Congrats! You've now finished your first transformer model! Since this is a totally new exercise, we would really appreciate it if you could give us some [feedback](https://forms.gle/ZUZKcBiSY7bpsQko9)! Like which explanations did you like or not like, what was to hard and maybe what was to easy! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dbbe05",
   "metadata": {},
   "source": [
    "To create a zip file with your submission, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16ff7d9c337e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant folders: ['submission_files', 'exercise_code']\n",
      "notebooks files: ['2_transformer.ipynb', '1_dataset_and_collator.ipynb']\n",
      "Adding folder submission_files\n",
      "Adding folder exercise_code\n",
      "Adding notebook 2_transformer.ipynb\n",
      "Adding notebook 1_dataset_and_collator.ipynb\n",
      "Zipping successful! Zip is stored under: /u/home/chchin/i2dl/output/exercise12.zip\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.util.submit import submit_exercise\n",
    "\n",
    "path = os.path.join(root_path, 'output', 'exercise12')\n",
    "submit_exercise(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442471dcd3e8742",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "To complete the exercise, submit your final model to our submission portal - you probably know the procedure by now.\n",
    "\n",
    "1. Go on [our submission page](https://i2dl.cvg.cit.tum.de/submission/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum online and check your mails there. You will get an ID which we need in the next step.\n",
    "2. Log into [our submission page](https://i2dl.cvg.cit.tum.de/submission/) with your account details and upload the `zip` file. Once successfully uploaded, you should be able to see the submitted file selectable on the top.\n",
    "3. Click on this file and run the submission script. You will get an email with your score as well as a message if you have surpassed the threshold.\n",
    "\n",
    "# Submission Goals\n",
    "\n",
    "- Goal: Successfully implement a transformer model!\n",
    "\n",
    "- Points:\n",
    "    - 5 points per Module if shape is correct (FeedForwardNeuralNetwork, EncoderBlock, DecoderBlock, Transformer)\n",
    "    - 5 points per Module if output is correct (FeedForwardNeuralNetwork, EncoderBlock, DecoderBlock, Transformer)\n",
    "    - 30 points if submitted model reaches minimum score\n",
    "    - Total = 4 x 5 + 4 x 5 + 30 = 70\n",
    "\n",
    "- Passing Criteria: Minimum of 65 points!\n",
    "- Feel free to submit an unlimited number of assignments until the end of the semester; however, any submissions made after the deadline will not contribute to your bonus points.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
